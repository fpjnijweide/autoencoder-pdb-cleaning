{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython import get_ipython\n"]},{"cell_type":"markdown","metadata":{},"source":["  # Automated experiments\n","\n","  This file contains an extended version of the code shown in continuous_variable_training.ipynb. Several options are added, such as convolutional layers, Gaussian kernels, and the ability to use a variational autoencoder. Methods were written so that a queue of experiments can be made, while the amount of outputs is heavily reduced.\n","\n","  After running the experiments, the data was copied into a text editor, and using regular expressions, lines that do not contain curly brackets (such lines only contain information which is useful to see training progress) were removed. This data can be pasted into a program such as Google Sheets or Microsoft Excel, and split into multiple columns (like in a .csv file) using the semicolon as delimiter. The result of this can be seen in the .xlsx file included in this folder.\n","\n","  For detailed explanations on the code, please look at continuous_variable_training.ipynb\n","\n","  PS. these experiments will take several days to run on modern desktop computers. You might want to comment some of the lines in the second code cell to reduce the amount of measurements."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Settings\n","\n","USE_GPU = False\n","\n","TEST_RUN_EPOCHS = False  # Whether to force the number of epochs below. Useful to test for errors without having to wait for hours of training\n","TEST_RUN_EPOCH_NR = 1\n","LOAD_DATA = True  # Whether to add results to previous .csv data\n","verbosity = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","import os\n","\n","if USE_GPU:\n","    gpu_string = \"_gpu\"\n","else:\n","    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n","    gpu_string = \"\"\n","\n","import csv\n","import tensorflow as tf\n","import scipy.stats\n","import scipy.spatial\n","import numpy as np\n","import tensorflow.keras as keras\n","import pyAgrum as gum\n","import pandas as pd\n","import sklearn.model_selection\n","import math\n","import gc\n","import gkernel  # gkernel.py in this folder\n","import dill\n","from tensorflow.python.framework import ops\n","from tensorflow.python.ops import math_ops\n","\n","from IPython.display import clear_output\n","\n","print('Imports done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.python.client import device_lib\n","\n","print(device_lib.list_local_devices())\n","print(tf.__version__)\n","print(\"Num GPUs Available: \" + str(len(tf.config.experimental.list_physical_devices('GPU'))))\n","tf.test.is_gpu_available()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Default Variables for experiments.You probably want to use the cell above to change experiment vars.\n","BN_size_default = 3  # amount of BN variables, minimum 3\n","\n","mu_default = 0\n","sigma_default = 0.02  # Distribution of the noise\n","gaussian_noise_sigma_default = lambda SD: (0.01 / SD) * 100\n","sampling_density_default = 4  # How many bins the quasi-continuous variables use for distributing their probabilities. Higher_default = better approximation of continuous distributions\n","\n","activation_types_default = [keras.backend.sin, keras.backend.cos, keras.activations.linear, 'relu',\n","                            'swish']  # Activation layer types\n","hidden_layers_default = 3  # amount of hidden layers\n","encoding_dim_default = BN_size_default  # The dimensionality of the middle layer\n","loss_function_default = 'JSD'\n","training_method_default = 'semi'\n","\n","activity_regularizer_default = keras.regularizers.l2(10 ** -4)\n","activity_regularizer_default.__name__ = \"L2: 10^-4\"\n","\n","input_layer_type_default = 'gaussian_noise'\n","labeled_data_percentage_default = 2\n","\n","epochs_default = 100\n","VAE_default = False\n","CNN_default = False\n","kernel_landmarks_default = 100\n","\n","CNN_layers_default = 1\n","CNN_filters_default = 64\n","CNN_kernel_size_default = 3\n","\n","use_gaussian_noise_default = True\n","use_missing_entry_default = False\n","missing_entry_prob_default = 0.01\n","\n","defaults = dict(BN_size=BN_size_default, mu=mu_default, sigma=sigma_default, sampling_density=sampling_density_default,\n","                gaussian_noise_sigma=gaussian_noise_sigma_default, activation_types=activation_types_default,\n","                hidden_layers=hidden_layers_default, encoding_dim=encoding_dim_default,\n","                loss_function=loss_function_default, training_method=training_method_default,\n","                activity_regularizer=activity_regularizer_default, input_layer_type=input_layer_type_default,\n","                labeled_data_percentage=labeled_data_percentage_default, epochs=epochs_default, VAE=VAE_default,\n","                CNN=CNN_default, kernel_landmarks=kernel_landmarks_default, CNN_layers=CNN_layers_default,\n","                CNN_filters=CNN_filters_default, CNN_kernel_size=CNN_kernel_size_default,\n","                use_gaussian_noise=use_gaussian_noise_default, use_missing_entry=use_missing_entry_default,\n","                missing_entry_prob=missing_entry_prob_default)\n","\n","parameters = list(defaults.keys())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["experiments = []\n","experiment_config_list = []\n","experiment_config_strings = []\n","experiment_config_results = []\n","\n","\n","def load_from_csv(input_string):\n","    with open(input_string, 'r') as fp:\n","        reader = csv.reader(fp)\n","        li = list(reader)\n","    newlist = []\n","    for row in li:\n","        newrow = []\n","        for entry in row[1:]:\n","            if entry == '':\n","                break\n","            else:\n","                newrow.append(float(entry))\n","        newlist.append(newrow)\n","    return newlist\n","\n","\n","def find(inputlist, search, key=lambda z: z):\n","    for i in range(len(inputlist)):\n","        if key(inputlist[i]) == search:\n","            return i\n","    return None\n","\n","\n","def str_noneguard(obj):\n","    if hasattr(obj, '__name__'):\n","        return obj.__name__\n","    if obj is None:\n","        return ''\n","    if isinstance(obj, list):\n","        return str([str_noneguard(x) for x in obj])\n","    return str(obj)\n","\n","\n","def freeze(d):\n","    # thanks to https://stackoverflow.com/a/13264725\n","    if isinstance(d, dict):\n","        return frozenset((key, freeze(value)) for key, value in d.items())\n","    elif isinstance(d, list):\n","        return tuple(freeze(value) for value in d)\n","    return d\n","\n","\n","def gen_experiment(config_string, input_dict={}, parameter=None, vars=None):\n","    if parameter is None:\n","        vars = [None]\n","\n","    for x in vars:\n","        if (x is None or x == 'default') and (not parameter is None):\n","            x = defaults[parameter]\n","\n","        new_experiment_config = input_dict.copy()\n","        if parameter == 'input_layer_type' and x == 'VAE':\n","            new_experiment_config['VAE'] = True\n","        elif parameter == 'input_layer_type' and x == 'CNN':\n","            new_experiment_config['CNN'] = True\n","        elif parameter == 'missing_entry':\n","            new_experiment_config['use_missing_entry'] = True\n","            new_experiment_config['use_gaussian_noise'] = False\n","            new_experiment_config['missing_entry_prob'] = x\n","        elif parameter == 'missing_entry_combined':\n","            new_experiment_config['use_missing_entry'] = True\n","            new_experiment_config['use_gaussian_noise'] = True\n","            new_experiment_config['missing_entry_prob'] = x\n","        elif parameter == 'missing_entry_no_denoising':\n","            new_experiment_config['use_missing_entry'] = True\n","            new_experiment_config['use_gaussian_noise'] = False\n","            new_experiment_config['input_layer_type'] = 'dense'\n","            new_experiment_config['missing_entry_prob'] = x\n","        elif parameter == 'kernel_landmarks':\n","            new_experiment_config['input_layer_type'] = 'gaussian_kernel'\n","            new_experiment_config[parameter] = x\n","        elif parameter == 'CNN_kernel_size' or parameter == 'CNN_filters':\n","            new_experiment_config['CNN'] = True\n","            new_experiment_config[parameter] = x\n","        elif parameter == 'gaussian_noise_sigma':\n","            new_experiment_config['input_layer_type'] = 'gaussian_noise'\n","            new_experiment_config[parameter] = x\n","        elif parameter is not None:\n","            new_experiment_config[parameter] = x\n","\n","        full_string = str(config_string + \"    \" + str_noneguard(parameter) + \"    \" + str_noneguard(x))\n","\n","        if new_experiment_config in experiment_config_list:\n","            mapping = experiment_config_list.index(new_experiment_config)\n","        else:\n","            mapping = len(experiment_config_list)\n","            experiment_config_list.append(new_experiment_config)\n","            experiment_config_strings.append(full_string)\n","            experiment_config_results.append([])\n","\n","        experiments.append(\n","            {'config_string': config_string, 'input_dict': input_dict, 'parameter': parameter, 'vars': vars,\n","             'current_var': x, 'config': new_experiment_config, 'full_string': full_string, 'mapping': mapping})\n","\n","\n","ground_config_strings = [\"CCE, SD=4\", \"JSD, SD=4\", \"CCEu, SD=4\", \"JSDu, SD=4\", \"CCE, SD=100\", \"JSD, SD=100\",\n","                         \"CCEu, SD=100\", \"JSDu, SD=100\"]\n","\n","for config_string in ground_config_strings:\n","    ground_config = defaults.copy()\n","    if \"CCE\" in config_string:\n","        ground_config['loss_function'] = 'CCE'\n","    elif \"JSD\" in config_string:\n","        ground_config['loss_function'] = 'JSD'\n","    elif \"MSE\" in config_string:\n","        ground_config['loss_function'] = 'MSE'\n","    elif \"KLD\" in config_string:\n","        ground_config['loss_function'] = 'KLD'\n","\n","    if \"u,\" in config_string:\n","        ground_config['training_method'] = 'unsupervised'\n","    if \"SD=100\" in config_string:\n","        ground_config['sampling_density'] = 100\n","\n","    # 0\n","    if ground_config['training_method'] != 'unsupervised':\n","        gen_experiment(config_string, ground_config, 'training_method',\n","                       [\"supervised\", \"supervised_2_percent\", \"semi\", \"semi_sup_first\", \"semi_mixed\", \"unsupervised\"])\n","\n","    if ground_config['loss_function'] != 'CCE':\n","        # 1\n","        activation_list = [defaults['activation_types'], ['relu'], ['relu'] * 5,\n","                           [keras.backend.sin, keras.backend.cos, keras.activations.linear],\n","                           [keras.backend.sin, keras.backend.cos, keras.activations.linear, 'relu', 'sigmoid']]\n","        gen_experiment(config_string, ground_config, 'activation_types', activation_list)\n","\n","        # 2\n","        gen_experiment(config_string, ground_config, 'input_layer_type',\n","                       ['dense', 'gaussian_noise', 'gaussian_dropout', 'sqrt_softmax', 'gaussian_kernel', 'CNN', 'VAE'])\n","\n","        # 3\n","        gen_experiment(config_string, ground_config, 'encoding_dim', [2, 3, 6])\n","\n","        # 4\n","        gen_experiment(config_string, ground_config, 'hidden_layers', [3, 5, 7, 9, 27])\n","\n","        # 5\n","        regularizer_list = [None, keras.regularizers.l2(0.01), activity_regularizer_default,\n","                            keras.regularizers.l1(0.01), keras.regularizers.l1(10 ** -4)]\n","        regularizer_strings = [\"none\", \"L2: 0.01\", \"L2: 10^-4\", \"L1: 0.01\", \"L1: 10^-4\"]\n","        for i in range(len(regularizer_list)):\n","            try:\n","                regularizer_list[i].__name__ = regularizer_strings[i]\n","            except:\n","                pass\n","        gen_experiment(config_string, ground_config, 'activity_regularizer', regularizer_list)\n","\n","        sigma_list = [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\n","        gen_experiment(config_string, ground_config, 'sigma', sigma_list)\n","\n","        # 7\n","        if ground_config['sampling_density'] == 100:\n","            gen_experiment(config_string, ground_config, 'BN_size', [2, 3, 4, 5])\n","        else:\n","            gen_experiment(config_string, ground_config, 'BN_size', [2, 3, 4, 5, 10, 20, 30])\n","\n","    # 8\n","    if ground_config['training_method'] != 'unsupervised':\n","        gen_experiment(config_string, ground_config, 'labeled_data_percentage',\n","                       [99, 50, 20, 10, 5, 2, 1, 0.5, 0.25, 0.125, 0.05, 0.01])\n","    # 9\n","    if ground_config['sampling_density'] != 100:\n","        gen_experiment(config_string, ground_config, 'sampling_density', [4, 15, 25, 50, 100, 150, 300])\n","\n","    if ground_config['loss_function'] != 'CCE':\n","        # 10-13\n","\n","        gaussian_noise_sigma_strings = [\"lambda SD: 0.01\", \"lambda SD: 0.02\", \"lambda SD: 0.05\", \"lambda SD: 0.1\",\n","                                        \"lambda SD: 0.2\", \"lambda SD: (0.01/SD)*100\", \"lambda SD: (0.02/SD)*100\",\n","                                        \"lambda SD: (0.05/SD)*100\", \"lambda SD: (0.1/SD)*100\",\n","                                        \"lambda SD: (0.2/SD)*100\"]\n","        gaussian_noise_sigma_list = [lambda SD: 0.01, lambda SD: 0.02, lambda SD: 0.05, lambda SD: 0.1, lambda SD: 0.2,\n","                                     lambda SD: (0.01 / SD) * 100, lambda SD: (0.02 / SD) * 100,\n","                                     lambda SD: (0.05 / SD) * 100, lambda SD: (0.1 / SD) * 100,\n","                                     lambda SD: (0.2 / SD) * 100]\n","        for i in range(len(gaussian_noise_sigma_list)):\n","            gaussian_noise_sigma_list[i].__name__ = gaussian_noise_sigma_strings[i]\n","        gen_experiment(config_string, ground_config, 'gaussian_noise_sigma', gaussian_noise_sigma_list)\n","\n","        gen_experiment(config_string, ground_config, 'missing_entry', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\n","        gen_experiment(config_string, ground_config, 'missing_entry_combined', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\n","        gen_experiment(config_string, ground_config, 'missing_entry_no_denoising', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\n","\n","if LOAD_DATA:\n","    try:\n","        experiment_config_results = load_from_csv(\"experiment_config_results\" + gpu_string + \".csv\")\n","    except:\n","        print('could not load data')\n","\n","print(\"Experiments: \" + str(len(experiments)))\n","print(\"Experiment configs: \" + str(len(experiment_config_list)))\n","print(\"\\n\\n\\n----------DONE---------\\n\\n\\n\")\n","\n","for experiment in experiments:\n","    print(experiment['full_string'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# @dispatch.add_dispatch_support\n","def JSD(y_true, y_pred):\n","    #     y_pred = ops.convert_to_tensor_v2(y_pred)\n","    #     y_true = math_ops.cast(y_true, y_pred.dtype)\n","    #     y_true = keras.backend.clip(y_true, keras.backend.epsilon(), 1)\n","    #     y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1)\n","    y_pred = ops.convert_to_tensor_v2(y_pred)\n","    y_true = math_ops.cast(y_true, y_pred.dtype)\n","    y_true = keras.backend.clip(y_true, keras.backend.epsilon(), 1)\n","    y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1)\n","    means = 0.5 * (y_true + y_pred)\n","    divergence_tensor = 0.5 * keras.losses.kld(y_true, means) + 0.5 * keras.losses.kld(y_pred, means)\n","\n","    return divergence_tensor\n","\n","\n","def generate_samplespace(func, x_min, x_max, sampling_density):\n","    grid = np.linspace(x_min, x_max, sampling_density + 1)\n","    probs = np.diff(func.cdf(grid))\n","    return probs / np.sum(probs)  # ensuring it is normalized\n","\n","\n","def normalize_df(df):\n","    newdf = df.div(df.sum(axis=1), axis=0)\n","    SD = len(newdf.columns)\n","    return newdf.fillna(1 / SD)\n","\n","\n","def prob_distance(x, y):\n","    xt = x.T  # this is bad practice, but I cannot debug the statements below without doing this\n","    yt = y.T\n","    res = JSD(x, y)\n","    return res\n","\n","\n","def make_bn(BN_size, sampling_density):\n","    bn = gum.BayesNet(\"Quasi-Continuous\")\n","    a = bn.add(gum.LabelizedVariable(\"A\", \"A binary variable\", 2))\n","    bn.cpt(a)[:] = [0.4, 0.6]\n","\n","    if BN_size > 1:\n","        b = bn.add(gum.RangeVariable(\"B\", \"A range variable\", 0, sampling_density - 1))\n","        bn.addArc(a, b)\n","        first = generate_samplespace(scipy.stats.truncnorm(-10, 3), -10, 3, sampling_density)\n","        second = generate_samplespace(scipy.stats.truncnorm(-2, 6), -2, 6, sampling_density)\n","        bn.cpt(b)[{'A': 0}] = first\n","        bn.cpt(b)[{'A': 1}] = second\n","\n","    if BN_size > 2:\n","        c = bn.add(gum.RangeVariable(\"C\", \"Another quasi continuous variable\", 0, sampling_density - 1))\n","        bn.addArc(b, c)\n","        l = []\n","        for i in range(sampling_density):\n","            # the size and the parameter of gamma depends on the parent value\n","            k = (i * 30.0) / sampling_density\n","            l.append(generate_samplespace(scipy.stats.gamma(k + 1), 4, 5 + k, sampling_density))\n","        bn.cpt(c)[:] = l\n","\n","        for d in range(BN_size - 3):\n","            # new variable\n","            d = bn.add(gum.RangeVariable(\"D\" + str(d), \"Another quasi continuous variable\", 0, sampling_density - 1))\n","            l = []\n","            bn.addArc(c, d)\n","            for i in range(sampling_density):\n","                # the size and the parameter of gamma depends on the parent value\n","                k = (i * 30.0) / sampling_density\n","                l.append(generate_samplespace(scipy.stats.gamma(k + 1), 4, 5 + k, sampling_density))\n","            bn.cpt(d)[:] = l\n","\n","    return bn\n","\n","\n","def make_df(use_previous_df, bn, mu, sigma, use_gaussian_noise, use_missing_entry, missing_entry_prob):\n","    if use_previous_df:\n","        original_database = pd.read_csv(\"good_db.csv\")\n","    else:\n","        gum.generateCSV(bn, \"databases/database_original\" + gpu_string + \".csv\", 10000)\n","        original_database = pd.read_csv(\"databases/database_original\" + gpu_string + \".csv\")\n","    original_database = original_database.reindex(sorted(original_database.columns), axis=1)\n","    original_database.to_csv(\"databases/database_original\" + gpu_string + \".csv\")\n","\n","    size_dict = {}\n","    for column_name in original_database.columns:\n","        size_dict[column_name] = bn.variable(column_name).domainSize()\n","\n","    shape = [original_database.shape[0], sum(size_dict.values())]\n","\n","    df_cols_sorted = sorted(list(original_database.columns))\n","    sizes_sorted = [size_dict[x] for x in df_cols_sorted]\n","    sizes_sorted_with_leading_zero = [0] + sizes_sorted\n","\n","    data = np.ones(original_database.shape[0] * original_database.shape[1])\n","    row = list(range(original_database.shape[0])) * original_database.shape[1]\n","    col = []\n","    for i in range(original_database.values.T.shape[0]):\n","        for item in original_database.values.T[i]:\n","            col.append(item + sum(sizes_sorted_with_leading_zero[0:i + 1]))\n","    # print(col[20000])\n","\n","    input3 = scipy.sparse.coo_matrix((data, (row, col)), shape=tuple(shape)).todense()\n","\n","    first_id2 = df_cols_sorted[:]\n","    second_id2 = [list(range(x)) for x in sizes_sorted]\n","\n","    arrays3 = [np.repeat(first_id2, sizes_sorted), [item for sublist in second_id2 for item in sublist]]\n","    tuples2 = list(zip(*arrays3))\n","    index2 = pd.MultiIndex.from_tuples(tuples2, names=['Variable', 'Value'])\n","\n","    hard_evidence = pd.DataFrame(input3, columns=index2)\n","    hard_evidence.to_csv(\"databases/ground_truth\" + gpu_string + \".csv\")\n","\n","    df = hard_evidence + 0\n","\n","    if use_gaussian_noise:\n","        noise = np.random.normal(mu, sigma, hard_evidence.shape)\n","        df = df + noise\n","        df = df.clip(lower=0, upper=1)\n","    if use_missing_entry:\n","        # TODO FIX\n","        amount_of_variables = len(sizes_sorted)\n","        rows = len(df)\n","        total_entries = amount_of_variables * rows  # amount of probability distributions in the PDB\n","        missing_entry_nrs = np.random.choice(total_entries, size=round(total_entries * missing_entry_prob),\n","                                             replace=False)\n","        m = missing_entry_nrs[:]  # using an alias for shorter code\n","        col_index = 0\n","        for attribute_nr, size in enumerate(sizes_sorted):\n","            entries_this_col = m[(m >= rows * attribute_nr) & (m < rows * (attribute_nr + 1))]\n","            rows_this_col = entries_this_col - (rows * attribute_nr)\n","            df.iloc[rows_this_col, col_index:col_index + size] = 1\n","\n","            col_index += size\n","\n","    for col in df_cols_sorted:\n","        df[col] = normalize_df(df[col])\n","\n","    df.to_csv(\"databases/noisy_data\" + gpu_string + \".csv\")\n","\n","    return df, hard_evidence, sizes_sorted\n","\n","\n","class Sampling(keras.layers.Layer):\n","    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n","\n","    def call(self, inputs):\n","        z_mean, z_log_var = inputs\n","        batch = tf.shape(z_mean)[0]\n","        dim = tf.shape(z_mean)[1]\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n","\n","\n","class VAE_model(keras.Model):\n","    def __init__(self, encoder, decoder, loss_func, **kwargs):\n","        super(VAE_model, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.loss_func = loss_func\n","        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n","        self.reconstruction_loss_tracker = keras.metrics.Mean(\n","            name=\"reconstruction_loss\"\n","        )\n","        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [\n","            self.total_loss_tracker,\n","            self.reconstruction_loss_tracker,\n","            self.kl_loss_tracker,\n","        ]\n","\n","    def call(self, data):\n","        z_mean, z_log_var, z = self.encoder(data)\n","        reconstruction = self.decoder(z)\n","        return reconstruction\n","\n","    def train_step(self, data):\n","        with tf.GradientTape() as tape:\n","            x, y = data\n","            z_mean, z_log_var, z = self.encoder(x)\n","            reconstruction = self.decoder(z)\n","\n","            #             reconstruction_loss = self.loss_func(data, reconstruction)\n","\n","            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","            #             print(\"kl_loss before reduce mean\", kl_loss)\n","            #             print(\"kl_loss after reduce sum 1\", tf.reduce_sum(kl_loss, axis=1))\n","            #             print(\"kl_loss after reduce mean\", tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)))\n","            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","\n","            reconstruction_loss = tf.reduce_mean(\n","                self.loss_func(y, reconstruction)\n","            )\n","\n","            total_loss = reconstruction_loss + kl_loss\n","        grads = tape.gradient(total_loss, self.trainable_weights)\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","        return {\n","            \"loss\": self.total_loss_tracker.result(),\n","            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n","            \"kl_loss\": self.kl_loss_tracker.result(),\n","        }\n","\n","\n","def train_network(epochs, df, hard_evidence, activation_types, hidden_layers, encoding_dim, sizes_sorted, loss_function,\n","                  training_method, activity_regularizer, input_layer_type, labeled_data_percentage, VAE, CNN,\n","                  kernel_landmarks, CNN_layers, CNN_filters, CNN_kernel_size, gaussian_noise_sigma):\n","    x_train, y_train, x_train_nolabel = None, None, None\n","\n","    if training_method == 'supervised':\n","        x_train, y_train = df, hard_evidence\n","    elif training_method == \"unsupervised\":\n","        x_train = df\n","    elif training_method == \"supervised_2_percent\":\n","        x_train, _, y_train, _ = sklearn.model_selection.train_test_split(df, hard_evidence, test_size=0.98)\n","    elif training_method == \"semi\" or training_method == \"semi_supervised\" or training_method == \"semisupervised\" or training_method == \"semi_sup_first\" or training_method == \"semi_mixed\":\n","        x_train, x_train_nolabel, y_train, _ = sklearn.model_selection.train_test_split(df, hard_evidence, test_size=(\n","                                                                                                                             100 - labeled_data_percentage) / 100)  # semi supervised\n","    else:\n","        raise Exception(\"Invalid training method\")\n","\n","    x_train = np.float32(x_train)\n","    if y_train is not None:\n","        y_train = np.float32(y_train)\n","    if x_train_nolabel is not None:\n","        x_train_nolabel = np.float32(x_train_nolabel)\n","\n","    # types = ['relu','relu','relu','relu','relu']\n","    input_dim = sum(sizes_sorted)\n","    # this is our input placeholder\n","    #     input_layer = keras.layers.Input(shape=(None,input_dim))\n","    input_layer = keras.layers.Input(shape=(input_dim,))\n","\n","    # \"encoded\" is the encoded representation of the input\n","    if input_layer_type == 'dense' and not CNN:\n","        x = keras.layers.Dense(input_dim, activation='relu', activity_regularizer=activity_regularizer)(input_layer)\n","    elif CNN:\n","        x = tf.expand_dims(input_layer, axis=2)\n","        x = keras.layers.Conv1D(input_shape=(0, input_dim), filters=CNN_filters, kernel_size=CNN_kernel_size,\n","                                activation='relu')(x)\n","        x = keras.layers.MaxPooling1D(pool_size=2)(x)\n","        x = keras.layers.Flatten()(x)\n","\n","    elif input_layer_type == 'gaussian_noise':\n","        x = keras.layers.GaussianNoise(gaussian_noise_sigma)(input_layer)\n","    elif input_layer_type == 'gaussian_dropout':\n","        x = keras.layers.GaussianDropout(0.01)(input_layer)\n","    elif input_layer_type == 'sqrt_softmax':\n","        x = keras.layers.Lambda(keras.backend.sqrt)(input_layer)\n","        x = keras.layers.Softmax()(x)\n","    elif input_layer_type == \"gaussian_kernel\":\n","\n","        x = gkernel.GaussianKernel3(kernel_landmarks, input_dim)(input_layer)\n","\n","    encode_ratio = 0.1\n","    middle = hidden_layers // 2\n","    for i in range(hidden_layers):\n","        if CNN and i < CNN_layers - 1:\n","\n","            x = tf.expand_dims(x, axis=2)\n","            x = keras.layers.Conv1D(filters=CNN_filters, kernel_size=CNN_kernel_size, activation='relu')(x)\n","            x = keras.layers.MaxPooling1D(pool_size=2)(x)\n","            x = keras.layers.Flatten()(x)\n","\n","        elif VAE and i == middle:\n","\n","            z_mean = keras.layers.Dense(encoding_dim, name=\"z_mean\")(x)\n","            z_log_var = keras.layers.Dense(encoding_dim, name=\"z_log_var\")(x)\n","            z = Sampling()([z_mean, z_log_var])\n","            latent_inputs = keras.layers.Input(shape=(encoding_dim,))\n","            x = tf.add(latent_inputs, 0)\n","\n","        else:\n","            ratio = 2 ** (math.log2(encode_ratio) + abs(i - middle))\n","            size = encoding_dim if i == middle else max((min(input_dim, int(ratio * input_dim))), encoding_dim)\n","            #         print(size)\n","            if len(activation_types) > 1:\n","                x = keras.layers.concatenate(\n","                    [keras.layers.Dense(size, activation=type, activity_regularizer=activity_regularizer)(x) for type in\n","                     activation_types], axis=1)\n","            else:\n","                x = keras.layers.Dense(size, activation=activation_types[0], activity_regularizer=activity_regularizer)(\n","                    x)\n","\n","    final_layer_list = [keras.layers.Dense(size, activation='softmax', activity_regularizer=activity_regularizer)(x) for\n","                        size in sizes_sorted]\n","\n","    decoded = keras.layers.concatenate(final_layer_list, axis=1)\n","\n","\n","    if VAE:\n","        encoder = keras.models.Model(input_layer, [z_mean, z_log_var, z], name=\"encoder\")\n","        decoder = keras.models.Model(latent_inputs, decoded, name=\"decoder\")\n","        autoencoder = VAE_model(encoder, decoder, loss_function)\n","        autoencoder.compile(optimizer='adam', metrics=['accuracy'])  # semi supervised\n","    else:\n","        autoencoder = keras.models.Model(input_layer, outputs=decoded)\n","        autoencoder.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])  # semi supervised\n","\n","    hist = keras.callbacks.History()\n","\n","    if training_method == 'supervised' or training_method == \"supervised_2_percent\":\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"semi\" or training_method == \"semi_supervised\" or training_method == \"semisupervised\":\n","        # SEMI SUPERVISED\n","        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"unsupervised\":\n","        # UNSUPERVISED\n","        autoencoder.fit(x_train, x_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"semi_sup_first\":\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"semi_mixed\":\n","        for i in range(epochs):\n","            autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=1, batch_size=32, shuffle=True, callbacks=[hist],\n","                            verbose=verbosity)\n","            autoencoder.fit(x_train, y_train, epochs=1, batch_size=32, shuffle=True, callbacks=[hist],\n","                            verbose=verbosity)\n","    else:\n","        raise Exception(\"Invalid training method\")\n","    return autoencoder\n","\n","\n","def measure_performance(df, hard_evidence, autoencoder, sizes_sorted):\n","    test_data = df.head(10000)\n","\n","    verify_data = hard_evidence.iloc[test_data.index]\n","    results = pd.DataFrame(autoencoder.predict(test_data))\n","\n","    results.to_csv(\"databases/post_cleaning\" + gpu_string + \".csv\")\n","\n","    i = 0\n","    distances_before = []\n","    distances_after = []\n","    for size in sizes_sorted:\n","        dist_before = prob_distance(verify_data.iloc[:, i:i + size], test_data.iloc[:, i:i + size])\n","        dist_after = prob_distance(verify_data.iloc[:, i:i + size], results.iloc[:, i:i + size])\n","        distances_before.append(np.nansum(dist_before))\n","        distances_after.append(np.nansum(dist_after))\n","        i += size\n","\n","    avg_distance_before = np.nansum(distances_before)\n","    avg_distance_after = np.nansum(distances_after)\n","    noise_left = 100 * avg_distance_after / avg_distance_before\n","    return noise_left\n","\n","\n","def custom_loss(y_true, y_pred, sizes_sorted, loss_func):\n","    i = 0\n","    total_loss = 0\n","    if loss_func == 'JSD':\n","        loss_func = JSD\n","    elif loss_func == \"CCE\":\n","        loss_func = keras.losses.categorical_crossentropy\n","    else:\n","        loss_func = keras.losses.get(loss_func)\n","\n","    loss_list = []\n","\n","    for size in sizes_sorted:\n","        new_loss = loss_func(y_true[:, i:i + size], y_pred[:, i:i + size])\n","        loss_list.append(new_loss)\n","        i += size\n","    good_loss = tf.math.add_n(loss_list)\n","    return good_loss\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# -------------\n","def run_experiment(epochs=epochs_default, use_previous_df=False, BN_size=BN_size_default,\n","                   sampling_density=sampling_density_default, mu=mu_default, sigma=sigma_default,\n","                   activation_types=activation_types_default, hidden_layers=hidden_layers_default,\n","                   encoding_dim=encoding_dim_default, loss_function=loss_function_default,\n","                   training_method=training_method_default, activity_regularizer=activity_regularizer_default,\n","                   input_layer_type=input_layer_type_default, labeled_data_percentage=labeled_data_percentage_default,\n","                   VAE=VAE_default, CNN=CNN_default, kernel_landmarks=kernel_landmarks_default,\n","                   CNN_layers=CNN_layers_default, CNN_filters=CNN_filters_default,\n","                   CNN_kernel_size=CNN_kernel_size_default, gaussian_noise_sigma=gaussian_noise_sigma_default,\n","                   use_gaussian_noise=use_gaussian_noise_default, use_missing_entry=use_missing_entry_default,\n","                   missing_entry_prob=missing_entry_prob_default):\n","    if TEST_RUN_EPOCHS:\n","        epochs = TEST_RUN_EPOCH_NR\n","\n","    bn = make_bn(BN_size, sampling_density)\n","    sigma = (sigma / sampling_density) * 100\n","    gaussian_noise_sigma = gaussian_noise_sigma(sampling_density)\n","    df, hard_evidence, sizes_sorted = make_df(use_previous_df, bn, mu, sigma, use_gaussian_noise, use_missing_entry,\n","                                              missing_entry_prob)\n","\n","    if loss_function != 'MSE':\n","        old_loss = loss_function[:]\n","        loss_function = lambda y_true, y_pred: custom_loss(y_true, y_pred, sizes_sorted, old_loss)\n","\n","    autoencoder = train_network(epochs, df, hard_evidence, activation_types, hidden_layers, encoding_dim, sizes_sorted,\n","                                loss_function, training_method, activity_regularizer, input_layer_type,\n","                                labeled_data_percentage, VAE, CNN, kernel_landmarks, CNN_layers, CNN_filters,\n","                                CNN_kernel_size, gaussian_noise_sigma)\n","    noise_left = measure_performance(df, hard_evidence, autoencoder, sizes_sorted)\n","    result = 100 - noise_left\n","    del autoencoder\n","    gc.collect()\n","    keras.backend.clear_session()\n","    return result\n","\n","\n","runs = 0\n","lowest_results = 0\n","\n","while lowest_results < 10:\n","    lowest_results = min([len(x) for x in experiment_config_results])\n","    lowest_results_forcpu = min([len(experiment_config_results[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) < 100])\n","    lowest_results_forgpu = min([len(experiment_config_results[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) >= 100])\n","    highest_results = max([len(x) for x in experiment_config_results])\n","    print(\"\\n\\n----- LOWEST RESULTS: \" + str(lowest_results) + \", HIGHEST: \" + str(highest_results) + \" ------\\n\\n\")\n","    for i in (range(len(experiments))):\n","        experiment = experiments[i]\n","        x = experiment\n","        previous_runs = len(experiment_config_results[experiment['mapping']])\n","        if previous_runs == lowest_results:\n","            # if previous_runs==lowest_results_forcpu and (x['config']['sampling_density']*x['config']['BN_size'])<100:\n","            # if previous_runs==lowest_results_forgpu and (x['config']['sampling_density']*x['config']['BN_size'])>=100:\n","            if runs == 0:\n","                print(i)\n","            result = run_experiment(**experiment['config'])\n","            print(\"(\" + str(i) + \") \" + experiment['full_string'] + \";    \" + str(result))\n","            experiment_config_results[experiment['mapping']].append(result)\n","            #             experiment_configs_and_results[freeze(experiment['config'])].append(result)\n","            if runs % 10 == 0 and runs > 0:\n","                clear_output(wait=True)\n","            with open(\"experiments\" + gpu_string, \"wb\") as dill_file:\n","                dill.dump(experiments, dill_file)\n","            experiment_configs_csv = [[experiment_config_strings[i]] + experiment_config_results[i] for i in\n","                                      range(len(experiment_config_results))]\n","            with open(\"experiment_config_results\" + gpu_string + \".csv\", \"w\", newline=\"\") as f:\n","                writer = csv.writer(f)\n","                writer.writerows(experiment_configs_csv)\n","            runs += 1\n","    lowest_results = min([len(x) for x in experiment_config_results])\n","    lowest_results_forcpu = min([len(experiment_config_results[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) < 100])\n","    lowest_results_forgpu = min([len(experiment_config_results[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) >= 100])\n"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit (conda)","metadata":{"interpreter":{"hash":"acaf3325e2b9610722d5876fb194b771aa7b4a569003e8df92eaafe48eb5ed50"}}}}}