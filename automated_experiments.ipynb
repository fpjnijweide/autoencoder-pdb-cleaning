{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated experiments\n",
    "\n",
    "This file contains an extended version of the code shown in continuous_variable_training.ipynb. Several options are added, such as convolutional layers, Gaussian kernels, and the ability to use a variational autoencoder. Methods were written so that a queue of experiments can be made, while the amount of outputs is heavily reduced.\n",
    "\n",
    "After running the experiments, the data was copied into a text editor, and using regular expressions, lines that do not contain curly brackets (such lines only contain information which is useful to see training progress) were removed. This data can be pasted into a program such as Google Sheets or Microsoft Excel, and split into multiple columns (like in a .csv file) using the semicolon as delimiter. The result of this can be seen in the .xlsx file included in this folder.\n",
    "\n",
    "For detailed explanations on the code, please look at continuous_variable_training.ipynb\n",
    "\n",
    "PS. these experiments will take several days to run on modern desktop computers. You might want to comment some of the lines in the second code cell to reduce the amount of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "# from pylab import *\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "import scipy.stats\n",
    "import scipy.spatial\n",
    "import tensorflow as tf\n",
    "import pyAgrum as gum\n",
    "import pyAgrum.lib.notebook as gnb\n",
    "# from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import neural_structured_learning as nsl\n",
    "import math\n",
    "import gc\n",
    "import gkernel\n",
    "# import gkernel\n",
    "# import functools\n",
    "\n",
    "print('Imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_experiment(input_dict={},variable=None,vars=None):\n",
    "    if variable is None:\n",
    "        vars = [0]\n",
    "    experiment=[]\n",
    "    for x in vars:\n",
    "        new_experiment_config=input_dict.copy()\n",
    "        if variable is not None:\n",
    "            new_experiment_config[variable]=x\n",
    "        new_experiment_config['string']=str(new_experiment_config)\n",
    "        experiment.append(new_experiment_config)\n",
    "    return experiment\n",
    "\n",
    "experiments = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# JSD experiments\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'}))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'sampling_density',[4,15,25,50,100,150,300]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'hidden_layers',[3,5,7,9,27]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'encoding_dim',[2,3,6]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'activation_types',[ ['relu'],['relu','relu','relu','relu','relu'], [keras.backend.sin,keras.backend.cos,keras.activations.linear],[keras.backend.sin,keras.backend.cos,keras.activations.linear, 'relu', 'sigmoid']]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'activity_regularizer',[keras.regularizers.l2(0.01),keras.regularizers.l2(10e-5),keras.regularizers.l1(0.01),keras.regularizers.l1(10e-5)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'sigma',[0.01*4/4,0.01/4,0.01/math.sqrt(4),0.01*4/100,0.01/100,0.01/math.sqrt(100)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'input_layer_type',['gaussian_noise','gaussian_dropout','sqrt_softmax']))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'unlabeled_data_percentage',[1-(5/100),1-(1/100),1-(0.5/100),1-(0.25/100),1-(0.125/100)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'BN_size',[2,3,4,5]))\n",
    "# JSD experiments NB100\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100}))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'hidden_layers',[3,5,7,9,27]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'encoding_dim',[2,3,6]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'activation_types',[ ['relu'],['relu','relu','relu','relu','relu'], [keras.backend.sin,keras.backend.cos,keras.activations.linear],[keras.backend.sin,keras.backend.cos,keras.activations.linear, 'relu', 'sigmoid'] ]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'activity_regularizer',[keras.regularizers.l2(0.01),keras.regularizers.l2(10e-5),keras.regularizers.l1(0.01),keras.regularizers.l1(10e-5)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'sigma',[0.01*4/4,0.01/4,0.01/math.sqrt(4),0.01*4/100,0.01/100,0.01/math.sqrt(100)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'input_layer_type',['gaussian_noise','gaussian_dropout','sqrt_softmax']))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'unlabeled_data_percentage',[1-(5/100),1-(1/100),1-(0.5/100),1-(0.25/100),1-(0.125/100)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'BN_size',[2,3,4,5]))\n",
    "# JSD Gaussian kernel experiments\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','input_layer_type':'gaussian_kernel'},'kernel_landmarks',[25,50,100,200,500,1000]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'input_layer_type':'gaussian_kernel'},'kernel_landmarks',[25,50,100,200,500,1000]))\n",
    "# CCE Gaussian kernel experiments\n",
    "experiments.append(gen_experiment({'input_layer_type':'gaussian_kernel'},'kernel_landmarks',[25,50,100,200,500,1000]))\n",
    "experiments.append(gen_experiment({'sampling_density':100,'input_layer_type':'gaussian_kernel'},'kernel_landmarks',[25,50,100,200,500,1000]))\n",
    "\n",
    "\n",
    "# CCE experiments\n",
    "experiments.append(gen_experiment({}))\n",
    "experiments.append(gen_experiment({},'sampling_density',[4,15,25,50,100,150,300]))\n",
    "experiments.append(gen_experiment({},'hidden_layers',[3,5,7,9,27]))\n",
    "experiments.append(gen_experiment({},'encoding_dim',[2,3,6]))\n",
    "experiments.append(gen_experiment({},'activation_types',[ ['relu'],['relu','relu','relu','relu','relu'], [keras.backend.sin,keras.backend.cos,keras.activations.linear],[keras.backend.sin,keras.backend.cos,keras.activations.linear, 'relu', 'sigmoid']]))\n",
    "experiments.append(gen_experiment({},'activity_regularizer',[keras.regularizers.l2(0.01),keras.regularizers.l2(10e-5),keras.regularizers.l1(0.01),keras.regularizers.l1(10e-5)]))\n",
    "experiments.append(gen_experiment({},'sigma',[0.01*4/4,0.01/4,0.01/math.sqrt(4),0.01*4/100,0.01/100,0.01/math.sqrt(100)]))\n",
    "experiments.append(gen_experiment({},'input_layer_type',['gaussian_noise','gaussian_dropout','sqrt_softmax']))\n",
    "experiments.append(gen_experiment({},'unlabeled_data_percentage',[1-(5/100),1-(1/100),1-(0.5/100),1-(0.25/100),1-(0.125/100)]))\n",
    "experiments.append(gen_experiment({},'BN_size',[2,3,4,5]))\n",
    "# CCE experiments NB100\n",
    "experiments.append(gen_experiment({'sampling_density':100}))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'hidden_layers',[3,5,7,9,27]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'encoding_dim',[2,3,6]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'activation_types',[ ['relu'],['relu','relu','relu','relu','relu'], [keras.backend.sin,keras.backend.cos,keras.activations.linear],[keras.backend.sin,keras.backend.cos,keras.activations.linear, 'relu', 'sigmoid'] ]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'activity_regularizer',[keras.regularizers.l2(0.01),keras.regularizers.l2(10e-5),keras.regularizers.l1(0.01),keras.regularizers.l1(10e-5)]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'sigma',[0.01*4/4,0.01/4,0.01/math.sqrt(4),0.01*4/100,0.01/100,0.01/math.sqrt(100)]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'input_layer_type',['gaussian_noise','gaussian_dropout','sqrt_softmax']))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'unlabeled_data_percentage',[1-(5/100),1-(1/100),1-(0.5/100),1-(0.25/100),1-(0.125/100)]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'BN_size',[2,3,4,5]))\n",
    "\n",
    "# CCEu experiments\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'hidden_layers',[3,5,7,9,27]))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'encoding_dim',[2,3,6]))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'sigma',[0.01*4/4,0.01/4,0.01/math.sqrt(4),0.01*4/100,0.01/100,0.01/math.sqrt(100)]))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'input_layer_type',['gaussian_noise','gaussian_dropout','sqrt_softmax']))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'BN_size',[2,3,4,5]))\n",
    "\n",
    "# vergeten CCE\n",
    "experiments.append(gen_experiment({'sampling_density':100},'unlabeled_data_percentage',[1-(5/100),1-(1/100),1-(0.5/100),1-(0.25/100),1-(0.125/100)]))\n",
    "experiments.append(gen_experiment(sampling_density':100},'sigma',[0.01*4/4,0.01/4,0.01/math.sqrt(4),0.01*4/100,0.01/100,0.01/math.sqrt(100)]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'activation_types',[ ['relu','relu','relu','relu','relu'] ]))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised','input_layer_type':'gaussian_kernel'},'kernel_landmarks',[25,50,100,200,500,1000]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'hidden_layers',[3,5,7,9,27]))\n",
    "\n",
    "\n",
    "\n",
    "# CNN experiments\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','CNN':True},'CNN_kernel_size',[1,3,5,7,9]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','CNN':True},'CNN_filters',[16,32,64,128]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','CNN':True,'sampling_density':100},'CNN_kernel_size',[1,3,5,7,9]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','CNN':True,'sampling_density':100},'CNN_filters',[16,32,64,128]))\n",
    "\n",
    "\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','CNN':True,'sampling_density':100,'CNN_kernel_size':100}))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','CNN':True,'sampling_density':4,'CNN_kernel_size':4}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5x relu\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'activation_types',[ ['relu','relu','relu','relu','relu'] ]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'activation_types',[ ['relu','relu','relu','relu','relu'] ]))\n",
    "\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised','CNN':True},'CNN_kernel_size',[1,3,5,7,9,4]))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised','CNN':True},'CNN_filters',[16,32,64,128]))\n",
    "experiments.append(gen_experiment({'CNN':True},'CNN_kernel_size',[1,3,5,7,9,4]))\n",
    "experiments.append(gen_experiment({'CNN':True},'CNN_filters',[16,32,64,128]))\n",
    "experiments.append(gen_experiment({'CNN':True,'sampling_density':100},'CNN_kernel_size',[1,3,5,7,9,100]))\n",
    "experiments.append(gen_experiment({'CNN':True,'sampling_density':100},'CNN_filters',[16,32,64,128]))\n",
    "\n",
    "\n",
    "\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'BN_size',[10,20,30]))\n",
    "experiments.append(gen_experiment({},'BN_size',[2,3,4,5,10,20,30]))\n",
    "experiments.append(gen_experiment({},'unlabeled_data_percentage',[1-(5/100),1-(1/100),1-(0.5/100),1-(0.25/100),1-(0.125/100)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'BN_size',[10,20,30]))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'sampling_density',[4,15,25,50,100,150,300]))\n",
    "experiments.append(gen_experiment({},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi\",\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "\n",
    "# training method\n",
    "experiments.append(gen_experiment({},'training_method',[\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi\",\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'MSE'},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi\",\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "\n",
    "experiments.append(gen_experiment({'sampling_density':100},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi\",\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi\",\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'MSE','sampling_density':100},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi\",\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "\n",
    "# noise\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'sigma',[0.05,0.1,0.2]))\n",
    "experiments.append(gen_experiment({},'sigma',[0.05,0.1,0.2]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'sigma',[0.05,0.1,0.2]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'sigma',[0.05,0.1,0.2]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'sigma',[0.05,0.1,0.2]))\n",
    "\n",
    "#  VAE\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'VAE',[True]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'VAE',[True]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'VAE',[True]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'VAE',[True]))\n",
    "\n",
    "\n",
    "# JSD5\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'input_layer_type',['gaussian_noise','gaussian_dropout','sqrt_softmax','gaussian_kernel']))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'CNN':True}))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'VAE':True}))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'activity_regularizer',[keras.regularizers.l2(0.01),keras.regularizers.l2(10e-5),keras.regularizers.l1(0.01),keras.regularizers.l1(10e-5)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'activation_types',[ ['relu'],['relu','relu','relu','relu','relu'], [keras.backend.sin,keras.backend.cos,keras.activations.linear],[keras.backend.sin,keras.backend.cos,keras.activations.linear, 'relu', 'sigmoid']]))\n",
    "\n",
    "# further sampling density\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised'},'sampling_density',[150,300]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'sampling_density',[150,300]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'epochs':70},'sampling_density',[300]))\n",
    "\n",
    "\n",
    "# further jsd5\n",
    "experiments.append(gen_experiment({'sampling_density':100},'unlabeled_data_percentage',[1-(1/100)]))\n",
    "\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'encoding_dim',[2,3,6]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'sigma',[0.01*4/4,0.01/4,0.01/math.sqrt(4),0.01*4/100,0.01/100,0.01/math.sqrt(100),0.05,0.1,0.2]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'BN_size',[2,3,4,5]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'unlabeled_data_percentage',[1-(5/100),1-(1/100),1-(0.5/100),1-(0.25/100),1-(0.125/100)]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'input_layer_type':'gaussian_kernel'},'kernel_landmarks',[25,50,100,200,500,1000]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'CNN':True},'CNN_kernel_size',[1,3,5,7,9,100]))\n",
    "\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'input_layer_type':'gaussian_kernel','kernel_landmarks':25}))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'input_layer_type':'gaussian_kernel','kernel_landmarks':10}))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'input_layer_type':'gaussian_kernel','kernel_landmarks':3}))\n",
    "\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5,'CNN':True},'CNN_filters',[16,32,64,128]))\n",
    "\n",
    "\n",
    "# TODO further training method\n",
    "\n",
    "experiments.append(gen_experiment({},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi_sup_first\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD'},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi_sup_first\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'MSE'},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi_sup_first\"]))\n",
    "experiments.append(gen_experiment({'sampling_density':100},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi_sup_first\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi_sup_first\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'MSE','sampling_density':100},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi_sup_first\"]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'hidden_layers':5},'training_method',[\"supervised\",\"supervised_2_percent\",\"semi\",\"semi_sup_first\",\"semi_mixed\",\"unsupervised\"]))\n",
    "\n",
    "\n",
    "\n",
    "# more Gaussian kernel experiments\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','input_layer_type':'gaussian_kernel'},'kernel_landmarks',[3,10]))\n",
    "experiments.append(gen_experiment({'loss_function':'JSD','sampling_density':100,'input_layer_type':'gaussian_kernel'},'kernel_landmarks',[3,10]))\n",
    "experiments.append(gen_experiment({'input_layer_type':'gaussian_kernel'},'kernel_landmarks',[3,10]))\n",
    "experiments.append(gen_experiment({'sampling_density':100,'input_layer_type':'gaussian_kernel'},'kernel_landmarks',[3,10]))\n",
    "experiments.append(gen_experiment({'training_method':'unsupervised','input_layer_type':'gaussian_kernel'},'kernel_landmarks',[3,10]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ex in experiments:\n",
    "    for config in ex:\n",
    "        print(config['string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Default Variables for experiments.You probably want to use the cell above to change experiment vars.\n",
    "BN_size_default =3 # amount of BN variables, minimum 3\n",
    "\n",
    "mu_default, sigma_default = 0, 0.01 # Distribution of the noise\n",
    "sampling_density_default =4 # How many bins the quasi-continuous variables use for distributing their probabilities. Higher_default = better approximation of continuous distributions\n",
    "\n",
    "activation_types_default = [keras.backend.sin,keras.backend.cos,keras.activations.linear, 'relu', 'swish'] # Activation layer types\n",
    "hidden_layers_default =9 # amount of hidden layers\n",
    "encoding_dim_default = BN_size_default # The dimensionality of the middle layer\n",
    "loss_function_default = keras.losses.categorical_crossentropy\n",
    "training_method_default ='semi'\n",
    "activity_regularizer_default=None\n",
    "input_layer_type_default='dense'\n",
    "unlabeled_data_percentage_default=0.98\n",
    "\n",
    "epochs_default=100\n",
    "VAE_default = False\n",
    "CNN_default = False\n",
    "kernel_landmarks_default = 100\n",
    "\n",
    "CNN_layers_default = 1\n",
    "CNN_filters_default = 64\n",
    "CNN_kernel_size_default = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samplespace(function,x_min,x_max,measurements):\n",
    "    non_normalized = [function(x) for x in np.linspace(x_min,x_max,measurements)]\n",
    "    return (non_normalized / sum(non_normalized))\n",
    "\n",
    "def normalize_df(df):\n",
    "    return df.div(df.sum(axis=1), axis=0)\n",
    "#     return df.subtract(df.min(axis=1), axis=0).div(df.sum(axis=1), axis=0)\n",
    "\n",
    "def custom_loss2(y_true,y_pred,sizes_sorted):\n",
    "\n",
    "    i=0\n",
    "    total_loss = 0\n",
    "    for size in sizes_sorted:\n",
    "#         total_loss += keras.losses.kullback_leibler_divergence(y_true[:,i:i+size],y_pred[:,i:i+size])\n",
    "        total_loss += nsl.lib.jensen_shannon_divergence(y_true[:,i:i+size],y_pred[:,i:i+size],axis=1)\n",
    "        i+=size\n",
    "    return total_loss\n",
    "\n",
    "def prob_distance(x,y):\n",
    "    xt=x.T # this is bad practice, but I cannot debug the statements below without doing this\n",
    "    yt=y.T\n",
    "    res= scipy.spatial.distance.jensenshannon(xt,yt,base=2.0)\n",
    "#     print(res)\n",
    "    return res\n",
    "\n",
    "def make_bn(BN_size,sampling_density):\n",
    "    bn=gum.BayesNet(\"Quasi-Continuous\")\n",
    "    a=bn.add(gum.LabelizedVariable(\"A\",\"A binary variable\",2))\n",
    "    bn.cpt(a)[:]=[0.4, 0.6]\n",
    "\n",
    "    if BN_size>1:\n",
    "        b=bn.add(gum.RangeVariable(\"B\",\"A range variable\",0,sampling_density-1))\n",
    "        bn.addArc(a,b)\n",
    "        first = generate_samplespace(scipy.stats.norm().pdf,-10,3,sampling_density)\n",
    "        second = generate_samplespace(scipy.stats.norm().pdf,-2,6,sampling_density)\n",
    "        bn.cpt(b)[{'A':0}]=first\n",
    "        bn.cpt(b)[{'A':1}]=second\n",
    "\n",
    "\n",
    "    if BN_size>2:\n",
    "        c=bn.add(gum.RangeVariable(\"C\",\"Another quasi continuous variable\",0,sampling_density-1))\n",
    "        bn.addArc(b,c)\n",
    "        l=[]\n",
    "        for i in range(sampling_density):\n",
    "            # the size and the parameter of gamma depends on the parent value\n",
    "            k=(i*30.0)/sampling_density\n",
    "            l.append(generate_samplespace(scipy.stats.gamma(k+1).pdf,4,5+k,sampling_density))\n",
    "        bn.cpt(c)[:]=l\n",
    "\n",
    "\n",
    "        for d in range(BN_size-3):\n",
    "            # new variable\n",
    "            d=bn.add(gum.RangeVariable(\"D\"+str(d),\"Another quasi continuous variable\",0,sampling_density-1))\n",
    "            l=[]\n",
    "            bn.addArc(c,d)\n",
    "            for i in range(sampling_density):\n",
    "                # the size and the parameter of gamma depends on the parent value\n",
    "                k=(i*30.0)/sampling_density\n",
    "                l.append(generate_samplespace(scipy.stats.gamma(k+1).pdf,4,5+k,sampling_density))\n",
    "            bn.cpt(d)[:]=l\n",
    "\n",
    "    bn.cpt(a)[:]=[0.1,0.9]\n",
    "    return bn\n",
    "\n",
    "def make_df(use_previous_df,bn,mu,sigma):\n",
    "    if use_previous_df:\n",
    "        original_database=pd.read_csv(\"good_db.csv\")\n",
    "    else:\n",
    "        gum.generateCSV(bn,\"database.csv\",10000)\n",
    "        original_database=pd.read_csv(\"database.csv\")\n",
    "    original_database = original_database.reindex(sorted(original_database.columns), axis=1)\n",
    "\n",
    "\n",
    "    size_dict = {}\n",
    "    for column_name in original_database.columns:\n",
    "        size_dict[column_name] = bn.variable(column_name).domainSize()\n",
    "\n",
    "    shape = [original_database.shape[0],sum(size_dict.values())]\n",
    "\n",
    "    df_cols_sorted = sorted(list(original_database.columns))\n",
    "    sizes_sorted = [size_dict[x] for x in df_cols_sorted]\n",
    "    sizes_sorted_with_leading_zero = [0] + sizes_sorted\n",
    "\n",
    "    data=np.ones(original_database.shape[0]*original_database.shape[1])\n",
    "    row = list(range(original_database.shape[0]))*original_database.shape[1]\n",
    "    col = []\n",
    "    for i in range(original_database.values.T.shape[0]):\n",
    "        for item in original_database.values.T[i]:\n",
    "            col.append(item+sum(sizes_sorted_with_leading_zero[0:i+1]))\n",
    "    # print(col[20000])\n",
    "\n",
    "    \n",
    "    input3 = scipy.sparse.coo_matrix((data, (row, col)), shape=tuple(shape)).todense()\n",
    "\n",
    "    first_id2 = df_cols_sorted[:]\n",
    "    second_id2=[list(range(x)) for x in sizes_sorted]\n",
    "\n",
    "    arrays3=[np.repeat(first_id2,sizes_sorted) , [item for sublist in second_id2 for item in sublist]]\n",
    "    tuples2 = list(zip(*arrays3))\n",
    "    index2 = pd.MultiIndex.from_tuples(tuples2, names=['Variable', 'Value'])\n",
    "\n",
    "    hard_evidence = pd.DataFrame(input3,columns=index2)\n",
    "\n",
    "    noise = np.random.normal(mu, sigma, hard_evidence.shape) \n",
    "    df = hard_evidence + noise\n",
    "    df = df.clip(lower=0)\n",
    "    \n",
    "    for col in df_cols_sorted:\n",
    "        df[col] = normalize_df(df[col])\n",
    "    \n",
    "    return df, hard_evidence, sizes_sorted\n",
    "\n",
    "# def vae_loss(z_mean, z_log_var,loss_func):\n",
    "#     # adapted from https://stackoverflow.com/a/61945712\n",
    "#     def loss(y_true, y_pred):\n",
    "#         # mse loss\n",
    "#         reconstruction_loss = loss_func(y_true, y_pred)\n",
    "#         # kl loss\n",
    "#         kl_loss = 1 + z_log_var - keras.backend.square(z_mean) - keras.backend.exp(z_log_var)\n",
    "#         kl_loss = keras.backend.sum(kl_loss, axis=-1)\n",
    "#         kl_loss *= -0.5\n",
    "#         weight = 0.\n",
    "#         return reconstruction_loss + (weight * kl_loss)\n",
    "#     return loss\n",
    "\n",
    "# def sampling(args):\n",
    "#     # adapted from https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "#     z_mean, z_log_var = args\n",
    "#     #     batch = keras.backend.shape(z_mean)[0]\n",
    "#     # dim = keras.backend.int_shape(z_mean)[1]\n",
    "#     # epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
    "#     batch = z_mean.shape[0]\n",
    "#     dim = z_mean.shape[1]\n",
    "\n",
    "#     # by default, random_normal has mean = 0 and std = 1.0\n",
    "#     epsilon = keras.backend.random_normal(shape=(dim,))\n",
    "#     thing = z_mean + keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "#     return thing\n",
    "\n",
    "class KLDivergenceLayer(keras.layers.Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "    # Adapted from https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/\n",
    "    # I take no responsibility for this class\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * keras.backend.sum(1 + log_var -\n",
    "                                keras.backend.square(mu) -\n",
    "                                keras.backend.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(keras.backend.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs    \n",
    "\n",
    "def train_network(epochs,df,hard_evidence,activation_types,hidden_layers,encoding_dim,sizes_sorted,loss_function,training_method,activity_regularizer,input_layer_type,unlabeled_data_percentage,VAE,CNN,kernel_landmarks,CNN_layers,CNN_filters,CNN_kernel_size):\n",
    "    if training_method=='supervised' or training_method==\"unsupervised\":\n",
    "        x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(df,hard_evidence, test_size=0.2) #unsupervised\n",
    "    elif training_method==\"supervised_2_percent\":\n",
    "        x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(df,hard_evidence, test_size=0.98)\n",
    "    elif training_method==\"semi\" or training_method==\"semi_supervised\" or training_method==\"semisupervised\" or training_method==\"semi_sup_first\" or training_method==\"semi_mixed\":\n",
    "        x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(df,hard_evidence, test_size=unlabeled_data_percentage) # semi supervised\n",
    "        x_train_nolabel, x_test_nolabel, y_train_nolabel, y_test_nolabel = sklearn.model_selection.train_test_split(x_test,y_test,test_size=0.2) # semi supervised\n",
    "    else:\n",
    "        raise Exception(\"Invalid training method\")\n",
    "\n",
    "\n",
    "\n",
    "    x_train = np.float32(x_train)\n",
    "    x_test = np.float32(x_test)\n",
    "    y_train = np.float32(y_train)\n",
    "    y_test = np.float32(y_test)\n",
    "\n",
    "\n",
    "    # types = ['relu','relu','relu','relu','relu']\n",
    "    input_dim = sum(sizes_sorted)\n",
    "    # this is our input placeholder\n",
    "#     input_layer = keras.layers.Input(shape=(None,input_dim))\n",
    "    input_layer = keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    if input_layer_type =='dense' and not CNN:\n",
    "        x = keras.layers.Dense(input_dim, activation='relu',activity_regularizer=activity_regularizer)(input_layer)\n",
    "    elif CNN:\n",
    "        x = tf.expand_dims(input_layer, axis=2)\n",
    "        x = keras.layers.Conv1D(input_shape=(0,input_dim),filters=CNN_filters, kernel_size=CNN_kernel_size, activation='relu')(x)\n",
    "        x = keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = keras.layers.Flatten()(x)\n",
    "\n",
    "    elif input_layer_type == 'gaussian_noise':    \n",
    "        x = keras.layers.GaussianNoise(0.01)(input_layer)\n",
    "    elif input_layer_type == 'gaussian_dropout':\n",
    "        x = keras.layers.GaussianDropout(0.01)(input_layer)\n",
    "    elif input_layer_type == 'sqrt_softmax':\n",
    "        x = keras.layers.Lambda(keras.backend.sqrt)(input_layer)\n",
    "        x= keras.layers.Softmax()(x)\n",
    "    elif input_layer_type == \"gaussian_kernel\":\n",
    "        \n",
    "        x = gkernel.GaussianKernel3(kernel_landmarks, input_dim)(input_layer)\n",
    "\n",
    "\n",
    "    encode_ratio=0.1\n",
    "    middle = hidden_layers//2\n",
    "    hidden_layer_list = []\n",
    "    for i in range(hidden_layers):\n",
    "        if CNN and i < CNN_layers-1:\n",
    "            \n",
    "            x = tf.expand_dims(x, axis=2)\n",
    "            x = keras.layers.Conv1D(filters=CNN_filters, kernel_size=CNN_kernel_size, activation='relu')(x)\n",
    "            x = keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = keras.layers.Flatten()(x)\n",
    "            \n",
    "        elif VAE and i==middle:\n",
    "            \n",
    "            #Credits to https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/\n",
    "            z_mean = keras.layers.Dense(encoding_dim)(x)\n",
    "            z_log_var = keras.layers.Dense(encoding_dim)(x)\n",
    "            z_mean, z_log_var = KLDivergenceLayer()([z_mean, z_log_var])\n",
    "            z_sigma = keras.layers.Lambda(lambda t: keras.backend.exp(.5*t))(z_log_var)\n",
    "\n",
    "            eps = keras.layers.Input(tensor=keras.backend.random_normal(shape=(keras.backend.shape(x)[0], encoding_dim)))\n",
    "            z_eps = keras.layers.Multiply()([z_sigma, eps])\n",
    "            z = keras.layers.Add()([z_mean, z_eps])\n",
    "            \n",
    "\n",
    "        else:\n",
    "            ratio = 2**(math.log2(encode_ratio)+abs(i-middle))\n",
    "            size = encoding_dim if i==middle else max(( min(input_dim,int(ratio*input_dim))),encoding_dim)\n",
    "    #         print(size)\n",
    "            if len(activation_types)>1:\n",
    "                x = keras.layers.concatenate([keras.layers.Dense(size, activation=type,activity_regularizer=activity_regularizer)(x) for type in activation_types],axis=1)\n",
    "            else:\n",
    "                x = keras.layers.Dense(size, activation=activation_types[0],activity_regularizer=activity_regularizer)(x)\n",
    "        #     x = keras.layers.Dense(min(input_dim,int(ratio*input_dim)), activation='relu')(x)\n",
    "\n",
    "\n",
    "    final_layer_list= [keras.layers.Dense(size, activation='softmax',activity_regularizer=activity_regularizer)(x) for size in sizes_sorted]\n",
    "\n",
    "    decoded = keras.layers.concatenate(final_layer_list,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = keras.models.Model(input_layer, outputs=decoded)\n",
    "\n",
    "\n",
    "\n",
    "    hist = keras.callbacks.History()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "    if VAE:\n",
    "#         decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "#         decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "#         h_decoded = decoder_h(z)\n",
    "#         x_decoded_mean = decoder_mean(h_decoded)\n",
    "        \n",
    "#         # end-to-end autoencoder\n",
    "#         vae = Model(x, x_decoded_mean)\n",
    "\n",
    "        # encoder, from inputs to latent space\n",
    "        encoder = keras.models.Model(input_layer, outputs=z_mean)\n",
    "        \n",
    "\n",
    "\n",
    "        # generator, from latent space to reconstructed inputs\n",
    "        decoder_input = keras.layers.Input(shape=(encoding_dim,))\n",
    "        x = decoder_input\n",
    "        for i in range(middle+1,hidden_layers):\n",
    "            ratio = 2**(math.log2(encode_ratio)+abs(i-middle))\n",
    "            size = encoding_dim if i==middle else max(( min(input_dim,int(ratio*input_dim))),encoding_dim)\n",
    "    #         print(size)\n",
    "            if len(activation_types)>1:\n",
    "                x = keras.layers.concatenate([keras.layers.Dense(size, activation=type,activity_regularizer=activity_regularizer)(x) for type in activation_types],axis=1)\n",
    "            else:\n",
    "                x = keras.layers.Dense(size, activation=activation_types[0],activity_regularizer=activity_regularizer)(x)\n",
    "\n",
    "        final_layer_list_generator = [keras.layers.Dense(size, activation='softmax',activity_regularizer=activity_regularizer)(x) for size in sizes_sorted]\n",
    "        decoded_generator = keras.layers.concatenate(final_layer_list_generator,axis=1)\n",
    "        generator = keras.models.Model(decoder_input, outputs=decoded_generator)\n",
    "        \n",
    "#         old_loss_func = loss_function\n",
    "#         loss_function=vae_loss(z_mean, z_log_var,old_loss_func)\n",
    "#         autoencoder.compile(optimizer='adam',loss=loss_function, metrics=['accuracy'],run_eagerly=True) \n",
    "#     else:\n",
    "    autoencoder.compile(optimizer='adam',loss=loss_function, metrics=['accuracy']) #semi supervised\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    if training_method=='supervised' or training_method==\"supervised_2_percent\":\n",
    "        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, validation_data=(x_test, y_test), callbacks=[hist],verbose=1)\n",
    "    elif training_method==\"semi\" or training_method==\"semi_supervised\" or training_method==\"semisupervised\":\n",
    "        # SEMI SUPERVISED\n",
    "        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, validation_data=(x_test_nolabel, x_test_nolabel), callbacks=[hist],verbose=1)\n",
    "        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, validation_data=(x_test, y_test), callbacks=[hist],verbose=1)\n",
    "    elif training_method==\"unsupervised\":\n",
    "        # UNSUPERVISED\n",
    "        autoencoder.fit(x_train, x_train, epochs=epochs, batch_size=32, shuffle=True, validation_data=(x_test, x_test), callbacks=[hist],verbose=1)\n",
    "    elif training_method==\"semi_sup_first\":\n",
    "        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, validation_data=(x_test, y_test), callbacks=[hist],verbose=1)\n",
    "        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, validation_data=(x_test_nolabel, x_test_nolabel), callbacks=[hist],verbose=1)\n",
    "    elif training_method==\"semi_mixed\":\n",
    "        for i in range(epochs):\n",
    "            autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=1, batch_size=32, shuffle=True, validation_data=(x_test_nolabel, x_test_nolabel), callbacks=[hist],verbose=1)\n",
    "            autoencoder.fit(x_train, y_train, epochs=1, batch_size=32, shuffle=True, validation_data=(x_test, y_test), callbacks=[hist],verbose=1)\n",
    "    else:\n",
    "        raise Exception(\"Invalid training method\")\n",
    "    return autoencoder\n",
    "        \n",
    "def measure_performance(df, hard_evidence, autoencoder,sizes_sorted):\n",
    "    test_data = df.head(10000)\n",
    "    verify_data = hard_evidence.iloc[test_data.index]\n",
    "    results = pd.DataFrame(autoencoder.predict(test_data))\n",
    "\n",
    "\n",
    "\n",
    "    i=0\n",
    "    distances_before=[]\n",
    "    distances_after=[]\n",
    "    for size in sizes_sorted:\n",
    "        dist_before = prob_distance(verify_data.iloc[:,i:i+size],test_data.iloc[:,i:i+size])\n",
    "        dist_after = prob_distance(verify_data.iloc[:,i:i+size],results.iloc[:,i:i+size])\n",
    "        distances_before.append(np.nansum(dist_before)/len(dist_before))\n",
    "        distances_after.append(np.nansum(dist_after)/len(dist_after))\n",
    "        i+=size\n",
    "\n",
    "    avg_distance_before = np.nansum([sizes_sorted[i]*distances_before[i] for i in range(len(sizes_sorted))])/sum(sizes_sorted)\n",
    "    avg_distance_after = np.nansum([sizes_sorted[i]*distances_after[i] for i in range(len(sizes_sorted))])/sum(sizes_sorted)\n",
    "    improvement = avg_distance_before - avg_distance_after\n",
    "    noise_left = 100*avg_distance_after/avg_distance_before\n",
    "    return noise_left    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------------\n",
    "def run_experiment(string=\"\",epochs=epochs_default,use_previous_df=False,BN_size=BN_size_default,sampling_density=sampling_density_default,mu=mu_default,sigma=sigma_default,activation_types=activation_types_default,hidden_layers=hidden_layers_default,encoding_dim=encoding_dim_default,loss_function=loss_function_default,training_method=training_method_default,activity_regularizer=activity_regularizer_default,input_layer_type=input_layer_type_default,unlabeled_data_percentage=unlabeled_data_percentage_default, VAE=VAE_default,CNN=CNN_default,kernel_landmarks=kernel_landmarks_default,CNN_layers=CNN_layers_default,CNN_filters=CNN_filters_default,CNN_kernel_size=CNN_kernel_size_default):\n",
    "    bn = make_bn(BN_size,sampling_density)\n",
    "    df, hard_evidence, sizes_sorted = make_df(use_previous_df,bn,mu,sigma)\n",
    "\n",
    "    if loss_function=='JSD':\n",
    "        loss_function = lambda y_true,y_pred: custom_loss2(y_true,y_pred,sizes_sorted)\n",
    "\n",
    "    autoencoder =train_network(epochs,df,hard_evidence,activation_types,hidden_layers,encoding_dim,sizes_sorted,loss_function,training_method,activity_regularizer,input_layer_type,unlabeled_data_percentage, VAE,CNN,kernel_landmarks,CNN_layers,CNN_filters,CNN_kernel_size)\n",
    "    noise_left = measure_performance(df, hard_evidence, autoencoder,sizes_sorted)\n",
    "    print(string+ \"; \" + str(100-noise_left))\n",
    "    del autoencoder\n",
    "    gc.collect()\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "def all_experiments(experiments):\n",
    "    for experiment_configs in experiments:\n",
    "        print(\"{------- NEW EXPERIMENT ---------}; \")\n",
    "        for experiment_config in experiment_configs:\n",
    "#             print(experiment_config)\n",
    "            run_experiment(**experiment_config)\n",
    "            \n",
    "all_experiments(experiments)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
