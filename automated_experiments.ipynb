{"cells":[{"cell_type":"markdown","source":["    # Automated experiments\r\n","\r\n","    This file runs all the experiments described in the paper. It will keep running until there are n=10 measurements for every configuration, which might take a few days."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Settings\r\n","\r\n","USE_GPU = False # Turn on to use GPU and to append \"_gpu\" behind all saved files\r\n","\r\n","TEST_RUN_EPOCHS = False  # Whether to force the number of epochs below. Useful to test for errors without having to wait for hours of training\r\n","TEST_RUN_EPOCH_NR = 1\r\n","LOAD_DATA = True  # Whether to add results to previously saved .csv data\r\n","verbosity = 0\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import os\r\n","\r\n","if USE_GPU:\r\n","    gpu_string = \"_gpu\"\r\n","else:\r\n","    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n","    gpu_string = \"\"\r\n","\r\n","try:\r\n","    from IPython.display import display\r\n","    get_ipython().run_line_magic('matplotlib', 'inline')\r\n","except:\r\n","    display = print\r\n","\r\n","import csv\r\n","import tensorflow as tf\r\n","import scipy.stats\r\n","import scipy.spatial\r\n","import numpy as np\r\n","import tensorflow.keras as keras\r\n","import pyAgrum as gum\r\n","import pandas as pd\r\n","import sklearn.model_selection\r\n","import math\r\n","import gc\r\n","import gkernel  # gkernel.py in this folder\r\n","import csv_to_pdb #csv_to_pdb.py in this folder\r\n","import dill\r\n","import signal\r\n","import logging\r\n","from tensorflow.python.framework import ops\r\n","from tensorflow.python.ops import math_ops\r\n","\r\n","from IPython.display import clear_output\r\n","\r\n","print('Imports done')\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from tensorflow.python.client import device_lib\r\n","\r\n","print(device_lib.list_local_devices())\r\n","print(tf.__version__)\r\n","print(\"Num GPUs Available: \" + str(len(tf.config.experimental.list_physical_devices('GPU'))))\r\n","tf.test.is_gpu_available()\r\n","\r\n","def clean_directory():\r\n","    dir_name = os.getcwd()\r\n","    test = os.listdir(dir_name + \"/input_data/\")\r\n","\r\n","    for item in test:\r\n","        if item.endswith(\".pdb\") or item.endswith(\".df\") or item.endswith(\".pkl\"):\r\n","            os.remove(os.path.join(dir_name, item))\r\n","\r\n","def clean(*args):\r\n","    clean_directory()\r\n","    os._exit(0)\r\n","\r\n","for sig in (signal.SIGABRT, signal.SIGINT, signal.SIGTERM):\r\n","    signal.signal(sig, clean)\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# # Default Variables for experiments.You probably want to use the cell above to change experiment vars.\r\n","BN_size_default = 3  # amount of BN variables, minimum 3\r\n","\r\n","mu_default = 0\r\n","sigma_default = 0.02  # Distribution of the noise\r\n","gaussian_noise_sigma_default = lambda SD: (0.01 / SD) * 100\r\n","sampling_density_default = None  # How many bins the quasi-continuous variables use for distributing their probabilities. Higher_default = better approximation of continuous distributions\r\n","\r\n","activation_types_default = [keras.backend.sin, keras.backend.cos, keras.activations.linear, 'relu',\r\n","                            'swish']  # Activation layer types\r\n","hidden_layers_default = 3  # amount of hidden layers\r\n","encoding_dim_default = BN_size_default  # The dimensionality of the middle layer\r\n","loss_function_default = 'JSD'\r\n","training_method_default = 'semi'\r\n","\r\n","activity_regularizer_default = keras.regularizers.l2(10 ** -4)\r\n","activity_regularizer_default.__name__ = \"L2, 10^-4\"\r\n","\r\n","input_layer_type_default = 'gaussian_noise'\r\n","labeled_data_percentage_default = 2\r\n","\r\n","epochs_default = 100\r\n","VAE_default = False\r\n","CNN_default = False\r\n","kernel_landmarks_default = 100\r\n","\r\n","CNN_layers_default = 1\r\n","CNN_filters_default = 64\r\n","CNN_kernel_size_default = 3\r\n","\r\n","use_gaussian_noise_default = True\r\n","use_missing_entry_default = False\r\n","missing_entry_prob_default = 0.01\r\n","rows_default=10000\r\n","use_file_default=None\r\n","\r\n","defaults = dict(BN_size=BN_size_default, mu=mu_default, sigma=sigma_default, sampling_density=sampling_density_default,\r\n","                gaussian_noise_sigma=gaussian_noise_sigma_default, activation_types=activation_types_default,\r\n","                hidden_layers=hidden_layers_default, encoding_dim=encoding_dim_default,\r\n","                loss_function=loss_function_default, training_method=training_method_default,\r\n","                activity_regularizer=activity_regularizer_default, input_layer_type=input_layer_type_default,\r\n","                labeled_data_percentage=labeled_data_percentage_default, epochs=epochs_default, VAE=VAE_default,\r\n","                CNN=CNN_default, kernel_landmarks=kernel_landmarks_default, CNN_layers=CNN_layers_default,\r\n","                CNN_filters=CNN_filters_default, CNN_kernel_size=CNN_kernel_size_default,\r\n","                use_gaussian_noise=use_gaussian_noise_default, use_missing_entry=use_missing_entry_default,\r\n","                missing_entry_prob=missing_entry_prob_default,rows=rows_default,use_file=use_file_default)\r\n","\r\n","parameters = list(defaults.keys())\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["experiments = []\r\n","experiment_config_list = []\r\n","experiment_config_strings = []\r\n","\r\n","\r\n","experiment_config_JSD_before = []\r\n","experiment_config_JSD_after = []\r\n","\r\n","# noise reduction performance score: (noise after) / (noise before) in %\r\n","\r\n","experiment_config_flip_TP = [] # good flip\r\n","experiment_config_flip_TN = [] # good left unflipped\r\n","experiment_config_flip_FP = [] # unneeded flip\r\n","experiment_config_flip_FN = [] # wrong flip or unflipped\r\n","\r\n","# wrong attributes before = TP + FN\r\n","# correct attributes before = FP + TN\r\n","# wrong attributes after = FP + FN\r\n","# correct attributes after = TP + TN\r\n","\r\n","# REDUCTION IN WRONG VARIABLES = (FP + FN) / (TP + FN)\r\n","\r\n","# accuracy = TP + TN / (TP+TN+FP+FN)\r\n","# f1 score = (2TP) + (2TP + FP + FN)\r\n","\r\n","experiment_config_entropy_before = []\r\n","experiment_config_entropy_after = []\r\n","\r\n","# allows us to see whether total uncertainty increases or decreases\r\n","# entropy reduction score: H_before / H_after\r\n","\r\n","# we can then see if these correlate: is there an increase/decrease in performance when entropy is increased?\r\n","# how about f1/acc?\r\n","\r\n","\r\n","\r\n","class DelayedKeyboardInterrupt:\r\n","    # Original author: Gary van der Merwe at https://stackoverflow.com/a/21919644\r\n","    def __enter__(self):\r\n","        self.signal_received = False\r\n","        self.old_handler = signal.signal(signal.SIGINT, self.handler)\r\n","                \r\n","    def handler(self, sig, frame):\r\n","        self.signal_received = (sig, frame)\r\n","        logging.debug('SIGINT received. Delaying KeyboardInterrupt.')\r\n","    \r\n","    def __exit__(self, type, value, traceback):\r\n","        signal.signal(signal.SIGINT, self.old_handler)\r\n","        if self.signal_received:\r\n","            self.old_handler(*self.signal_received)\r\n","\r\n","def load_from_csv(input_string):\r\n","    with open(input_string, 'r') as fp:\r\n","        reader = csv.reader(fp)\r\n","        li = list(reader)\r\n","    newlist = []\r\n","    for row in li:\r\n","        newrow = []\r\n","        for entry in row[1:]:\r\n","            if entry == '':\r\n","                break\r\n","            else:\r\n","                newrow.append(float(entry))\r\n","        newlist.append(newrow)\r\n","    return newlist\r\n","\r\n","\r\n","def find(inputlist, search, key=lambda z: z):\r\n","    for i in range(len(inputlist)):\r\n","        if key(inputlist[i]) == search:\r\n","            return i\r\n","    return None\r\n","\r\n","\r\n","def str_noneguard(obj):\r\n","    if hasattr(obj, '__name__'):\r\n","        return obj.__name__\r\n","    if obj is None:\r\n","        return ''\r\n","    if isinstance(obj, list):\r\n","        return str([str_noneguard(x) for x in obj])\r\n","    return str(obj)\r\n","\r\n","\r\n","def freeze(d):\r\n","    # thanks to https://stackoverflow.com/a/13264725\r\n","    if isinstance(d, dict):\r\n","        return frozenset((key, freeze(value)) for key, value in d.items())\r\n","    elif isinstance(d, list):\r\n","        return tuple(freeze(value) for value in d)\r\n","    return d\r\n","\r\n","\r\n","def gen_experiment(config_string, input_dict={}, parameter=None, vars=None):\r\n","    if parameter is None:\r\n","        vars = [None]\r\n","\r\n","    for x in vars:\r\n","        if (not parameter=='activity_regularizer') and (x is None or x == 'default') and (not parameter is None):\r\n","            x = defaults[parameter]\r\n","\r\n","        new_experiment_config = input_dict.copy()\r\n","        if parameter == 'input_layer_type' and x == 'VAE':\r\n","            new_experiment_config['VAE'] = True\r\n","        elif parameter == 'input_layer_type' and x == 'CNN':\r\n","            new_experiment_config['CNN'] = True\r\n","        elif parameter == 'missing_entry':\r\n","            new_experiment_config['use_missing_entry'] = True\r\n","            new_experiment_config['use_gaussian_noise'] = False\r\n","            new_experiment_config['missing_entry_prob'] = x\r\n","        elif parameter == 'missing_entry_combined':\r\n","            new_experiment_config['use_missing_entry'] = True\r\n","            new_experiment_config['use_gaussian_noise'] = True\r\n","            new_experiment_config['missing_entry_prob'] = x\r\n","        elif parameter == 'missing_entry_no_denoising':\r\n","            new_experiment_config['use_missing_entry'] = True\r\n","            new_experiment_config['use_gaussian_noise'] = False\r\n","            new_experiment_config['input_layer_type'] = 'dense'\r\n","            new_experiment_config['missing_entry_prob'] = x\r\n","        elif parameter == 'kernel_landmarks':\r\n","            new_experiment_config['input_layer_type'] = 'gaussian_kernel'\r\n","            new_experiment_config[parameter] = x\r\n","        elif parameter == 'CNN_kernel_size' or parameter == 'CNN_filters':\r\n","            new_experiment_config['CNN'] = True\r\n","            new_experiment_config[parameter] = x\r\n","        elif parameter == 'gaussian_noise_sigma':\r\n","            new_experiment_config['input_layer_type'] = 'gaussian_noise'\r\n","            new_experiment_config[parameter] = x\r\n","        elif parameter is not None:\r\n","            new_experiment_config[parameter] = x\r\n","\r\n","        full_string = str(config_string + \"    \" + str_noneguard(parameter) + \"    \" + str_noneguard(x))\r\n","        full_string_list = (config_string,str_noneguard(parameter),str_noneguard(x))\r\n","\r\n","        if new_experiment_config in experiment_config_list:\r\n","            mapping = experiment_config_list.index(new_experiment_config)\r\n","        else:\r\n","            mapping = len(experiment_config_list)\r\n","            experiment_config_list.append(new_experiment_config)\r\n","            experiment_config_strings.append(full_string)\r\n","            \r\n","            experiment_config_JSD_before.append([])\r\n","            experiment_config_JSD_after.append([])\r\n","\r\n","            experiment_config_flip_TP.append([])\r\n","            experiment_config_flip_TN.append([])\r\n","            experiment_config_flip_FP.append([])\r\n","            experiment_config_flip_FN.append([])\r\n","\r\n","            experiment_config_entropy_before.append([])\r\n","            experiment_config_entropy_after.append([])\r\n","\r\n","        experiments.append(\r\n","            {'config_string': config_string, 'input_dict': input_dict, 'parameter': parameter, 'vars': vars,\r\n","             'current_var': x, 'config': new_experiment_config, 'full_string': full_string, 'mapping': mapping, 'full_string_list':full_string_list})\r\n","\r\n","ground_config_strings_real_data_real_world_example = [\"JSDu, surgical_case_durations NO_ADDED_NOISE\",\"JSDu, LBP RA NO_ADDED_NOISE\",\r\n","\"JSDu, surgical_case_durations NO_ADDED_NOISE SD=4\",\"JSDu, LBP RA NO_ADDED_NOISE SD=4\",\r\n","\"JSDu, surgical_case_durations NO_ADDED_NOISE SD=100\",\"JSDu, LBP RA NO_ADDED_NOISE SD=100\"]\r\n","\r\n","ground_config_strings_real_data = [\"JSD, surgical_case_durations\",\"JSDu, surgical_case_durations\",\"JSD, LBP RA\",\"JSDu, LBP RA\",\r\n","\"JSD, surgical_case_durations, SD=4\",\"JSDu, surgical_case_durations, SD=4\",\"JSD, LBP RA, SD=4\",\"JSDu, LBP RA, SD=4\",\r\n","\"JSD, surgical_case_durations, SD=100\",\"JSDu, surgical_case_durations, SD=100\",\"JSD, LBP RA, SD=100\",\"JSDu, LBP RA, SD=100\"]\r\n","\r\n","ground_config_strings_synthetic = [\"CCE, SD=4\", \"JSD, SD=4\", \"CCEu, SD=4\", \"JSDu, SD=4\", \"CCE, SD=100\", \"JSD, SD=100\",\r\n","                         \"CCEu, SD=100\", \"JSDu, SD=100\"]\r\n","\r\n","ground_config_strings = ground_config_strings_real_data_real_world_example + ground_config_strings_real_data + ground_config_strings_synthetic\r\n","\r\n","for config_string in ground_config_strings:\r\n","    ground_config = defaults.copy()\r\n","    if \"CCE\" in config_string:\r\n","        ground_config['loss_function'] = 'CCE'\r\n","    elif \"JSD\" in config_string:\r\n","        ground_config['loss_function'] = 'JSD'\r\n","    elif \"MSE\" in config_string:\r\n","        ground_config['loss_function'] = 'MSE'\r\n","    elif \"KLD\" in config_string:\r\n","        ground_config['loss_function'] = 'KLD'\r\n","\r\n","    if \"u,\" in config_string:\r\n","        ground_config['training_method'] = 'unsupervised'\r\n","\r\n","    if \"SD=100\" in config_string:\r\n","        ground_config['sampling_density'] = 100\r\n","    elif \"SD=4\" in config_string:\r\n","        ground_config['sampling_density'] = 4\r\n","    \r\n","    if \"surgical_case_durations\" in config_string:\r\n","        ground_config['use_file']=\"input_data/surgical_case_durations.csv\"\r\n","    elif \"LBP RA\" in config_string:\r\n","        ground_config['use_file']=\"input_data/Dataset - LBP RA.csv\"\r\n","\r\n","    if ground_config['use_file'] is not None:\r\n","        if \"NO_ADDED_NOISE\" in config_string:\r\n","            # do not add noise, we work directly on ground truth\r\n","            gen_experiment(config_string, ground_config, 'sigma', [0])\r\n","        else:\r\n","            # If we are using a file\r\n","            sigma_list = [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\r\n","            gen_experiment(config_string, ground_config, 'sigma', sigma_list)\r\n","            gen_experiment(config_string, ground_config, 'missing_entry', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","            gen_experiment(config_string, ground_config, 'missing_entry_combined', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","    else:\r\n","    # 0\r\n","        if ground_config['training_method'] != 'unsupervised':\r\n","            gen_experiment(config_string, ground_config, 'training_method',\r\n","                        [\"supervised\", \"supervised_2_percent\", \"semi\", \"semi_sup_first\", \"semi_mixed\", \"unsupervised\"])\r\n","\r\n","        if ground_config['loss_function'] != 'CCE':\r\n","            # 1\r\n","            activation_list = [defaults['activation_types'], ['relu'], ['relu'] * 5,\r\n","                            [keras.backend.sin, keras.backend.cos, keras.activations.linear],\r\n","                            [keras.backend.sin, keras.backend.cos, keras.activations.linear, 'relu', 'sigmoid']]\r\n","            gen_experiment(config_string, ground_config, 'activation_types', activation_list)\r\n","\r\n","            # 2\r\n","            gen_experiment(config_string, ground_config, 'input_layer_type',\r\n","                        ['dense', 'gaussian_noise', 'gaussian_dropout', 'sqrt_softmax', 'gaussian_kernel', 'CNN', 'VAE'])\r\n","\r\n","            # 3\r\n","            gen_experiment(config_string, ground_config, 'encoding_dim', [2, 3, 6])\r\n","\r\n","            # 4\r\n","            gen_experiment(config_string, ground_config, 'hidden_layers', [3, 5, 7, 9, 27])\r\n","\r\n","            # 5\r\n","            regularizer_list = [None, keras.regularizers.l2(0.01), activity_regularizer_default,\r\n","                                keras.regularizers.l1(0.01), keras.regularizers.l1(10 ** -4)]\r\n","            regularizer_strings = [\"none\", \"L2, 0.01\", \"L2, 10^-4\", \"L1, 0.01\", \"L1, 10^-4\"]\r\n","            for i in range(len(regularizer_list)):\r\n","                try:\r\n","                    regularizer_list[i].__name__ = regularizer_strings[i]\r\n","                except:\r\n","                    pass\r\n","            gen_experiment(config_string, ground_config, 'activity_regularizer', regularizer_list)\r\n","\r\n","            sigma_list = [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\r\n","            gen_experiment(config_string, ground_config, 'sigma', sigma_list)\r\n","\r\n","            # 7\r\n","            if ground_config['sampling_density'] == 100:\r\n","                gen_experiment(config_string, ground_config, 'BN_size', [2, 3, 4, 5])\r\n","            else:\r\n","                gen_experiment(config_string, ground_config, 'BN_size', [2, 3, 4, 5, 10, 20, 30])\r\n","\r\n","        # 8\r\n","        if ground_config['training_method'] != 'unsupervised':\r\n","            gen_experiment(config_string, ground_config, 'labeled_data_percentage',\r\n","                        [99, 50, 20, 10, 5, 2, 1, 0.5, 0.25, 0.125, 0.05, 0.01])\r\n","        # 9\r\n","        if ground_config['sampling_density'] != 100:\r\n","            gen_experiment(config_string, ground_config, 'sampling_density', [4, 15, 25, 50, 100, 150, 300])\r\n","\r\n","        if ground_config['loss_function'] != 'CCE':\r\n","            # 10-13\r\n","            gaussian_noise_sigma_strings = [\"lambda SD, 0.01\", \"lambda SD, 0.02\", \"lambda SD, 0.05\", \"lambda SD, 0.1\",\r\n","                                            \"lambda SD, 0.2\", \"lambda SD, (0.01 over SD) cdot 100\", \"lambda SD, (0.02 over SD) cdot 100\",\r\n","                                            \"lambda SD, (0.05 over SD) cdot 100\", \"lambda SD, (0.1 over SD) cdot 100\",\r\n","                                            \"lambda SD, (0.2 over SD) cdot 100\"]\r\n","            gaussian_noise_sigma_list = [lambda SD: 0.01, lambda SD: 0.02, lambda SD: 0.05, lambda SD: 0.1, lambda SD: 0.2,\r\n","                                        lambda SD: (0.01 / SD) * 100, lambda SD: (0.02 / SD) * 100,\r\n","                                        lambda SD: (0.05 / SD) * 100, lambda SD: (0.1 / SD) * 100,\r\n","                                        lambda SD: (0.2 / SD) * 100]\r\n","            for i in range(len(gaussian_noise_sigma_list)):\r\n","                gaussian_noise_sigma_list[i].__name__ = gaussian_noise_sigma_strings[i]\r\n","            gen_experiment(config_string, ground_config, 'gaussian_noise_sigma', gaussian_noise_sigma_list)\r\n","\r\n","            gen_experiment(config_string, ground_config, 'missing_entry', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","            gen_experiment(config_string, ground_config, 'missing_entry_combined', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","            gen_experiment(config_string, ground_config, 'missing_entry_no_denoising', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","            #14\r\n","            gen_experiment(config_string, ground_config, 'rows', [10**2,10**3,10**4,10**5,10**6])\r\n","\r\n","\r\n","if LOAD_DATA:\r\n","    try:\r\n","        experiment_config_JSD_before = load_from_csv(\"results/experiment_config_JSD_before\" + gpu_string + \".csv\")\r\n","        experiment_config_JSD_after = load_from_csv(\"results/experiment_config_JSD_after\" + gpu_string + \".csv\")\r\n","\r\n","        experiment_config_flip_TP = load_from_csv(\"results/experiment_config_flip_TP\" + gpu_string + \".csv\")\r\n","        experiment_config_flip_TN = load_from_csv(\"results/experiment_config_flip_TN\" + gpu_string + \".csv\")\r\n","        experiment_config_flip_FP = load_from_csv(\"results/experiment_config_flip_FP\" + gpu_string + \".csv\")\r\n","        experiment_config_flip_FN = load_from_csv(\"results/experiment_config_flip_FN\" + gpu_string + \".csv\")\r\n","\r\n","        experiment_config_entropy_before = load_from_csv(\"results/experiment_config_entropy_before\" + gpu_string + \".csv\")\r\n","        experiment_config_entropy_after = load_from_csv(\"results/experiment_config_entropy_after\" + gpu_string + \".csv\")\r\n","    except:\r\n","        print('could not load data')\r\n","\r\n","print(\"Experiments: \" + str(len(experiments)))\r\n","print(\"Experiment configs: \" + str(len(experiment_config_list)))\r\n","print(\"\\n\\n\\n----------DONE---------\\n\\n\\n\")\r\n","\r\n","with pd.option_context(\"display.max_rows\", 1000):\r\n","    display(pd.DataFrame(pd.DataFrame(experiments).loc[:,\"full_string_list\"].values.tolist()))\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# @dispatch.add_dispatch_support\r\n","def JSD(y_true, y_pred):\r\n","    #     y_pred = ops.convert_to_tensor_v2(y_pred)\r\n","    #     y_true = math_ops.cast(y_true, y_pred.dtype)\r\n","    #     y_true = keras.backend.clip(y_true, keras.backend.epsilon(), 1)\r\n","    #     y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1)\r\n","    y_pred = ops.convert_to_tensor_v2(y_pred)\r\n","    y_true = math_ops.cast(y_true, y_pred.dtype)\r\n","    y_true = keras.backend.clip(y_true, keras.backend.epsilon(), 1)\r\n","    y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1)\r\n","    means = 0.5 * (y_true + y_pred)\r\n","    divergence_tensor = 0.5 * keras.losses.kld(y_true, means) + 0.5 * keras.losses.kld(y_pred, means)\r\n","\r\n","    return divergence_tensor\r\n","\r\n","\r\n","def generate_samplespace(func, x_min, x_max, sampling_density):\r\n","    grid = np.linspace(x_min, x_max, sampling_density + 1)\r\n","    probs = np.diff(func.cdf(grid))\r\n","    return probs / np.sum(probs)  # ensuring it is normalized\r\n","\r\n","\r\n","def normalize_df(df):\r\n","    newdf = df.div(df.sum(axis=1), axis=0)\r\n","    SD = len(newdf.columns)\r\n","    return newdf.fillna(1 / SD)\r\n","\r\n","\r\n","def prob_distance(x, y):\r\n","    xt = x.T  # this is bad practice, but I cannot debug the statements below without doing this\r\n","    yt = y.T\r\n","    res = JSD(x, y)\r\n","    return res\r\n","\r\n","\r\n","def make_bn(BN_size, sampling_density):\r\n","    bn = gum.BayesNet(\"Quasi-Continuous\")\r\n","    new_nodes = []\r\n","    # a = bn.add(gum.LabelizedVariable(\"A\", \"A binary variable\", 2))\r\n","    # bn.cpt(a)[:] = [0.4, 0.6]\r\n","    a = bn.add(gum.RangeVariable(\"A\", \"A range variable\", 0, sampling_density - 1))\r\n","    # bn.addArc(a, b)\r\n","    # first = generate_samplespace(scipy.stats.truncnorm(-10, 3), -10, 3, sampling_density)\r\n","    # second = generate_samplespace(scipy.stats.truncnorm(-2, 6), -2, 6, sampling_density)\r\n","    gauss = generate_samplespace(scipy.stats.norm(-2,2), -2, 2, sampling_density)\r\n","    # bn.cpt(b)[{'A': 0}] = first\r\n","    # bn.cpt(b)[{'A': 1}] = second\r\n","    bn.cpt(a)[:] = gauss\r\n","    new_nodes.append(a)\r\n","\r\n","    if BN_size > 1:\r\n","        b = bn.add(gum.RangeVariable(\"B\", \"Another quasi continuous variable\", 0, sampling_density - 1))\r\n","        bn.addArc(a, b)\r\n","        first = generate_samplespace(scipy.stats.truncnorm(-10, 3), -10, 3, sampling_density)\r\n","        second = generate_samplespace(scipy.stats.truncnorm(-2, 6), -2, 6, sampling_density)\r\n","        bn.cpt(b)[{'A': 0}] = first\r\n","        bn.cpt(b)[{'A': 1}] = second\r\n","        new_nodes.append(b)\r\n","\r\n","    if BN_size > 2:\r\n","        c = bn.add(gum.RangeVariable(\"C\", \"Another quasi continuous variable\", 0, sampling_density - 1))\r\n","        bn.addArc(b, c)\r\n","        l = []\r\n","        for i in range(sampling_density):\r\n","            # the size and the parameter of gamma depends on the parent value\r\n","            k = (i * 30.0) / sampling_density\r\n","            l.append(generate_samplespace(scipy.stats.gamma(k + 1), 4, 5 + k, sampling_density))\r\n","        bn.cpt(c)[:] = l\r\n","        new_nodes.append(c)\r\n","\r\n","        \r\n","        for d in range(BN_size - 3):\r\n","            # new variable\r\n","            new_nodes.append(bn.add(gum.RangeVariable(\"D\" + str(d), \"Another quasi continuous variable\", 0, sampling_density - 1)))\r\n","            l = []\r\n","            bn.addArc(new_nodes[-2], new_nodes[-1])\r\n","            for i in range(sampling_density):\r\n","                # the size and the parameter of gamma depends on the parent value\r\n","                k = (i * 30.0) / sampling_density\r\n","                l.append(generate_samplespace(scipy.stats.gamma(k + 1), 4, 5 + k, sampling_density))\r\n","            bn.cpt(new_nodes[-1])[:] = l\r\n","\r\n","    return bn\r\n","\r\n","\r\n","def make_df(use_file, bn, mu, sigma, use_gaussian_noise, use_missing_entry, missing_entry_prob,rows,full_string,sampling_density,gaussian_noise_layer_sigma):\r\n","    bins = None\r\n","    if not os.path.exists(\"output_data/\" + full_string+\"/\"):\r\n","        os.makedirs(\"output_data/\" + full_string+\"/\")\r\n","\r\n","    if use_file is not None:\r\n","        filename_no_extension = os.path.splitext(use_file)[0]\r\n","        if sampling_density is not None:\r\n","            SD_string = str(sampling_density)\r\n","        else:\r\n","            SD_string = \"None\"\r\n","        \r\n","        try:\r\n","            # try to load the files as we might have already generated hard evidence in earlier runs (and this takes a long, long time for proper databases)\r\n","            original_database = pd.read_pickle(filename_no_extension+\" SD=\"+SD_string+\".df\")\r\n","            hard_evidence = pd.read_pickle(filename_no_extension+\" SD=\"+SD_string+\".pdb\")\r\n","            sizes_sorted = list(pd.read_pickle(filename_no_extension+\" sizes SD=\"+SD_string+\".pkl\"))\r\n","            bins = list(pd.read_pickle(filename_no_extension+\" bins SD=\"+SD_string+\".pkl\"))\r\n","        except:\r\n","            original_database,sizes_sorted,hard_evidence,bins = csv_to_pdb.make_pdb(use_file,sampling_density)\r\n","            original_database.to_pickle(filename_no_extension+\" SD=\"+SD_string+\".df\")\r\n","            hard_evidence.to_pickle(filename_no_extension+\" SD=\"+SD_string+\".pdb\")\r\n","            pd.Series(sizes_sorted).to_pickle(filename_no_extension+\" sizes SD=\"+SD_string+\".pkl\")\r\n","            pd.Series(bins).to_pickle(filename_no_extension+\" bins SD=\"+SD_string+\".pkl\")\r\n","        df_cols_sorted = original_database.columns\r\n","    else:\r\n","        gum.generateCSV(bn, \"output_data/\" + full_string + \"/database_original\" + gpu_string + \".csv\", rows)\r\n","        original_database = pd.read_csv(\"output_data/\" + full_string + \"/database_original\" + gpu_string + \".csv\")\r\n","        original_database = original_database.reindex(sorted(original_database.columns), axis=1)\r\n","    original_database.to_csv(\"output_data/\" + full_string + \"/database_original\" + gpu_string + \".csv\")\r\n","\r\n","    if use_file is not None:\r\n","        pass\r\n","    else:\r\n","        size_dict = {}\r\n","        for column_name in original_database.columns:\r\n","            size_dict[column_name] = bn.variable(column_name).domainSize()\r\n","\r\n","        shape = [original_database.shape[0], sum(size_dict.values())]\r\n","\r\n","        df_cols_sorted = sorted(list(original_database.columns))\r\n","        sizes_sorted = [size_dict[x] for x in df_cols_sorted]\r\n","        sizes_sorted_with_leading_zero = [0] + sizes_sorted\r\n","\r\n","        data = np.ones(original_database.shape[0] * original_database.shape[1])\r\n","        row = list(range(original_database.shape[0])) * original_database.shape[1]\r\n","        col = []\r\n","        for i in range(original_database.values.T.shape[0]):\r\n","            for item in original_database.values.T[i]:\r\n","                col.append(item + sum(sizes_sorted_with_leading_zero[0:i + 1]))\r\n","\r\n","        input3 = scipy.sparse.coo_matrix((data, (row, col)), shape=tuple(shape)).todense()\r\n","\r\n","        first_id2 = df_cols_sorted[:]\r\n","        second_id2 = [list(range(x)) for x in sizes_sorted]\r\n","\r\n","        arrays3 = [np.repeat(first_id2, sizes_sorted), [item for sublist in second_id2 for item in sublist]]\r\n","        tuples2 = list(zip(*arrays3))\r\n","        index2 = pd.MultiIndex.from_tuples(tuples2, names=['Variable', 'Value'])\r\n","\r\n","        hard_evidence = pd.DataFrame(input3, columns=index2)\r\n","    hard_evidence.to_csv(\"output_data/\" + full_string + \"/ground_truth\" + gpu_string + \".csv\")\r\n","\r\n","    df = hard_evidence + 0\r\n","\r\n","    \r\n","\r\n","    # sigma = (sigma / sampling_density) * 100\r\n","\r\n","    sigmas = []\r\n","    gaussian_noise_layer_sigmas = []\r\n","    for attribute_size in sizes_sorted:\r\n","        sigmas.append(((sigma*np.ones(attribute_size)) /attribute_size) * 100)\r\n","        gaussian_noise_layer_sigmas.append(np.ones(attribute_size)*gaussian_noise_layer_sigma(attribute_size))\r\n","        # gaussian_noise_layer_sigmas.append(gaussian_noise_layer_sigma(attribute_size))\r\n","    \r\n","    sigmas_per_col = np.concatenate(sigmas)\r\n","    gaussian_noise_layer_sigma_new = np.concatenate(gaussian_noise_layer_sigmas)\r\n","\r\n","    noise_columns = [np.random.normal(mu, scale=s, size=(hard_evidence.shape[0])) for s in sigmas_per_col]\r\n","    noise = np.vstack(noise_columns).T\r\n","\r\n","    # TODO rework how gaussian noise is added in the network, maybe need a custom layer \r\n","\r\n","\r\n","    if use_gaussian_noise:\r\n","        df = df + noise\r\n","        df = df.clip(lower=0, upper=1)\r\n","    if use_missing_entry:\r\n","        amount_of_variables = len(sizes_sorted)\r\n","        rows = len(df)\r\n","        total_entries = amount_of_variables * rows  # amount of probability distributions in the PDB\r\n","        missing_entry_nrs = np.random.choice(total_entries, size=round(total_entries * missing_entry_prob),\r\n","                                             replace=False)\r\n","        m = missing_entry_nrs[:]  # using an alias for shorter code\r\n","        col_index = 0\r\n","        for attribute_nr, size in enumerate(sizes_sorted):\r\n","            entries_this_col = m[(m >= rows * attribute_nr) & (m < rows * (attribute_nr + 1))]\r\n","            rows_this_col = entries_this_col - (rows * attribute_nr)\r\n","            df.iloc[rows_this_col, col_index:col_index + size] = 1\r\n","\r\n","            col_index += size\r\n","\r\n","    for col in df_cols_sorted:\r\n","        df[col] = normalize_df(df[col])\r\n","\r\n","    df.to_csv(\"output_data/\" + full_string + \"/noisy_data\" + gpu_string + \".csv\")\r\n","\r\n","    return df, hard_evidence, sizes_sorted,gaussian_noise_layer_sigma_new,original_database,bins\r\n","\r\n","\r\n","class Sampling(keras.layers.Layer):\r\n","    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\r\n","\r\n","    def call(self, inputs):\r\n","        z_mean, z_log_var = inputs\r\n","        batch = tf.shape(z_mean)[0]\r\n","        dim = tf.shape(z_mean)[1]\r\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n","\r\n","\r\n","from tensorflow.python.ops import array_ops\r\n","from tensorflow.python.ops import math_ops\r\n","from tensorflow.python.keras.utils import tf_utils\r\n","\r\n","class GaussianNoisePerNeuron(keras.layers.Layer):\r\n","    # adapted from standard Keras GaussianNoise layer\r\n","    def __init__(self, stddev, **kwargs):\r\n","        super(GaussianNoisePerNeuron, self).__init__(**kwargs)\r\n","        self.supports_masking = True\r\n","        self.stddev = stddev\r\n","\r\n","    def call(self, inputs, training=None):\r\n","\r\n","        def noised():          \r\n","            # noise_columns = [np.random.normal(0, scale=s, size=(array_ops.shape(inputs)[0])) for s in self.stddev]\r\n","            # noise = np.vstack(noise_columns).T\r\n","            batch_size=array_ops.shape(inputs)[0]\r\n","            vector_length=len(self.stddev)\r\n","            noise = tf.squeeze(tf.stack([keras.backend.random_normal(shape=[batch_size,1],mean=0.,stddev=s,dtype=inputs.dtype) for s in self.stddev],axis=1))\r\n","            # noise = keras.backend.random_normal(shape=array_ops.shape(inputs),mean=0.,stddev=self.stddev,dtype=inputs.dtype)\r\n","            output = inputs + noise\r\n","            output = tf.reshape(output,[-1,vector_length])\r\n","            # output = tf.ensure_shape(output, len(self.stddev))\r\n","            return output\r\n","            # return inputs\r\n","            \r\n","\r\n","\r\n","        return keras.backend.in_train_phase(noised, inputs, training=training)\r\n","\r\n","    def get_config(self):\r\n","        config = {'stddev': self.stddev}\r\n","        base_config = super(GaussianNoisePerNeuron, self).get_config()\r\n","        return dict(list(base_config.items()) + list(config.items()))\r\n","\r\n","    @tf_utils.shape_type_conversion\r\n","    def compute_output_shape(self, input_shape):\r\n","        return input_shape    \r\n","\r\n","class VAE_model(keras.Model):\r\n","    def __init__(self, encoder, decoder, loss_func, **kwargs):\r\n","        super(VAE_model, self).__init__(**kwargs)\r\n","        self.encoder = encoder\r\n","        self.decoder = decoder\r\n","        self.loss_func = loss_func\r\n","        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\r\n","        self.reconstruction_loss_tracker = keras.metrics.Mean(\r\n","            name=\"reconstruction_loss\"\r\n","        )\r\n","        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\r\n","\r\n","    @property\r\n","    def metrics(self):\r\n","        return [\r\n","            self.total_loss_tracker,\r\n","            self.reconstruction_loss_tracker,\r\n","            self.kl_loss_tracker,\r\n","        ]\r\n","\r\n","    def call(self, data):\r\n","        z_mean, z_log_var, z = self.encoder(data)\r\n","        reconstruction = self.decoder(z)\r\n","        return reconstruction\r\n","\r\n","    def train_step(self, data):\r\n","        with tf.GradientTape() as tape:\r\n","            x, y = data\r\n","            z_mean, z_log_var, z = self.encoder(x)\r\n","            reconstruction = self.decoder(z)\r\n","\r\n","            #             reconstruction_loss = self.loss_func(data, reconstruction)\r\n","\r\n","            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\r\n","\r\n","            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\r\n","\r\n","            reconstruction_loss = tf.reduce_mean(\r\n","                self.loss_func(y, reconstruction)\r\n","            )\r\n","\r\n","            total_loss = reconstruction_loss + kl_loss\r\n","        grads = tape.gradient(total_loss, self.trainable_weights)\r\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n","        self.total_loss_tracker.update_state(total_loss)\r\n","        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\r\n","        self.kl_loss_tracker.update_state(kl_loss)\r\n","        return {\r\n","            \"loss\": self.total_loss_tracker.result(),\r\n","            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\r\n","            \"kl_loss\": self.kl_loss_tracker.result(),\r\n","        }\r\n","\r\n","\r\n","def train_network(epochs, df, hard_evidence, activation_types, hidden_layers, encoding_dim, sizes_sorted, loss_function,\r\n","                  training_method, activity_regularizer, input_layer_type, labeled_data_percentage, VAE, CNN,\r\n","                  kernel_landmarks, CNN_layers, CNN_filters, CNN_kernel_size, gaussian_noise_sigma):             \r\n","    x_train, y_train, x_train_nolabel = None, None, None\r\n","\r\n","    if training_method == 'supervised':\r\n","        x_train, y_train = df, hard_evidence\r\n","    elif training_method == \"unsupervised\":\r\n","        x_train = df\r\n","    elif training_method == \"supervised_2_percent\":\r\n","        x_train, _, y_train, _ = sklearn.model_selection.train_test_split(df, hard_evidence, test_size=0.98)\r\n","    elif training_method == \"semi\" or training_method == \"semi_supervised\" or training_method == \"semisupervised\" or training_method == \"semi_sup_first\" or training_method == \"semi_mixed\":\r\n","        x_train, x_train_nolabel, y_train, _ = sklearn.model_selection.train_test_split(df, hard_evidence, test_size=(\r\n","                                                                                                                             100 - labeled_data_percentage) / 100)  # semi supervised\r\n","    else:\r\n","        raise Exception(\"Invalid training method\")\r\n","\r\n","    x_train = np.float32(x_train)\r\n","    if y_train is not None:\r\n","        y_train = np.float32(y_train)\r\n","    if x_train_nolabel is not None:\r\n","        x_train_nolabel = np.float32(x_train_nolabel)\r\n","\r\n","    # types = ['relu','relu','relu','relu','relu']\r\n","    input_dim = sum(sizes_sorted)\r\n","    # this is our input placeholder\r\n","    #     input_layer = keras.layers.Input(shape=(None,input_dim))\r\n","    input_layer = keras.layers.Input(shape=(input_dim,))\r\n","\r\n","    # \"encoded\" is the encoded representation of the input\r\n","    if input_layer_type == 'dense' and not CNN:\r\n","        x = keras.layers.Dense(input_dim, activation='relu', activity_regularizer=activity_regularizer)(input_layer)\r\n","    elif CNN:\r\n","        x = tf.expand_dims(input_layer, axis=2)\r\n","        x = keras.layers.Conv1D(input_shape=(0, input_dim), filters=CNN_filters, kernel_size=CNN_kernel_size,\r\n","                                activation='relu')(x)\r\n","        x = keras.layers.MaxPooling1D(pool_size=2)(x)\r\n","        x = keras.layers.Flatten()(x)\r\n","\r\n","    elif input_layer_type == 'gaussian_noise':\r\n","        x = GaussianNoisePerNeuron(gaussian_noise_sigma)(input_layer)\r\n","    elif input_layer_type == 'gaussian_dropout':\r\n","        x = keras.layers.GaussianDropout(0.01)(input_layer)\r\n","    elif input_layer_type == 'sqrt_softmax':\r\n","        x = keras.layers.Lambda(keras.backend.sqrt)(input_layer)\r\n","        x = keras.layers.Softmax()(x)\r\n","    elif input_layer_type == \"gaussian_kernel\":\r\n","\r\n","        x = gkernel.GaussianKernel3(kernel_landmarks, input_dim)(input_layer)\r\n","\r\n","    encode_ratio = 0.1\r\n","    middle = hidden_layers // 2\r\n","    for i in range(hidden_layers):\r\n","        if CNN and i < CNN_layers - 1:\r\n","\r\n","            x = tf.expand_dims(x, axis=2)\r\n","            x = keras.layers.Conv1D(filters=CNN_filters, kernel_size=CNN_kernel_size, activation='relu')(x)\r\n","            x = keras.layers.MaxPooling1D(pool_size=2)(x)\r\n","            x = keras.layers.Flatten()(x)\r\n","\r\n","        elif VAE and i == middle:\r\n","\r\n","            z_mean = keras.layers.Dense(encoding_dim, name=\"z_mean\")(x)\r\n","            z_log_var = keras.layers.Dense(encoding_dim, name=\"z_log_var\")(x)\r\n","            z = Sampling()([z_mean, z_log_var])\r\n","            latent_inputs = keras.layers.Input(shape=(encoding_dim,))\r\n","            x = tf.add(latent_inputs, 0)\r\n","\r\n","        else:\r\n","            ratio = 2 ** (math.log2(encode_ratio) + abs(i - middle))\r\n","            size = encoding_dim if i == middle else max((min(input_dim, int(ratio * input_dim))), encoding_dim)\r\n","            #         print(size)\r\n","            if len(activation_types) > 1:\r\n","                x = keras.layers.concatenate(\r\n","                    [keras.layers.Dense(size, activation=type, activity_regularizer=activity_regularizer)(x) for type in\r\n","                     activation_types], axis=1)\r\n","            else:\r\n","                x = keras.layers.Dense(size, activation=activation_types[0], activity_regularizer=activity_regularizer)(\r\n","                    x)\r\n","\r\n","    final_layer_list = [keras.layers.Dense(size, activation='softmax', activity_regularizer=activity_regularizer)(x) for\r\n","                        size in sizes_sorted]\r\n","\r\n","    decoded = keras.layers.concatenate(final_layer_list, axis=1)\r\n","\r\n","\r\n","    if VAE:\r\n","        encoder = keras.models.Model(input_layer, [z_mean, z_log_var, z], name=\"encoder\")\r\n","        decoder = keras.models.Model(latent_inputs, decoded, name=\"decoder\")\r\n","        autoencoder = VAE_model(encoder, decoder, loss_function)\r\n","        autoencoder.compile(optimizer='adam', metrics=['accuracy'])  # semi supervised\r\n","    else:\r\n","        autoencoder = keras.models.Model(input_layer, outputs=decoded)\r\n","        autoencoder.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])  # semi supervised\r\n","\r\n","    hist = keras.callbacks.History()\r\n","\r\n","    if training_method == 'supervised' or training_method == \"supervised_2_percent\":\r\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                        verbose=verbosity)\r\n","    elif training_method == \"semi\" or training_method == \"semi_supervised\" or training_method == \"semisupervised\":\r\n","        # SEMI SUPERVISED\r\n","        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                        verbose=verbosity)\r\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                        verbose=verbosity)\r\n","    elif training_method == \"unsupervised\":\r\n","        # UNSUPERVISED\r\n","        autoencoder.fit(x_train, x_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                        verbose=verbosity)\r\n","    elif training_method == \"semi_sup_first\":\r\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                        verbose=verbosity)\r\n","        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                        verbose=verbosity)\r\n","    elif training_method == \"semi_mixed\":\r\n","        for i in range(epochs):\r\n","            autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=1, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                            verbose=verbosity)\r\n","            autoencoder.fit(x_train, y_train, epochs=1, batch_size=32, shuffle=True, callbacks=[hist],\r\n","                            verbose=verbosity)\r\n","    else:\r\n","        raise Exception(\"Invalid training method\")\r\n","    return autoencoder\r\n","\r\n","\r\n","def measure_performance(df, hard_evidence, autoencoder, sizes_sorted,rows,full_string,original_database,bins):      \r\n","    test_data = df.head(rows)\r\n","\r\n","    verify_data = hard_evidence.iloc[test_data.index]\r\n","    results = pd.DataFrame(autoencoder.predict(test_data))\r\n","    \r\n","    results.to_csv(\"output_data/\" + full_string + \"/post_cleaning\" + gpu_string + \".csv\")\r\n","\r\n","    i = 0\r\n","    distances_before = []\r\n","    distances_after = []\r\n","    flip_TP, flip_TN, flip_FP, flip_FN = [],[],[],[]\r\n","    entropy_before, entropy_after = [],[]\r\n","\r\n","\r\n","\r\n","\r\n","    ######### regenerate \"bins\" variable so that we can regenerate the original db\r\n","\r\n","\r\n","\r\n","\r\n","    cleaned_database_non_pdb = pd.DataFrame().reindex_like(original_database)\r\n","\r\n","    for column_index,size in enumerate(sizes_sorted):\r\n","        ground_truth_attribute = verify_data.iloc[:, i:i + size]\r\n","        cleaned_attribute = results.iloc[:, i:i + size]\r\n","        dirty_attribute = test_data.iloc[:, i:i + size]\r\n","\r\n","        dist_before = prob_distance(ground_truth_attribute, dirty_attribute)\r\n","        dist_after = prob_distance(ground_truth_attribute, cleaned_attribute)\r\n","\r\n","        distances_before.append(np.nansum(dist_before))\r\n","        distances_after.append(np.nansum(dist_after))\r\n","\r\n","        # going back to actual data instead of probabilities to see if values changed\r\n","        ground_truth_val = np.argmax(ground_truth_attribute.values,1)\r\n","        clean_val = np.argmax(cleaned_attribute.values,1)\r\n","        dirty_val = np.argmax(dirty_attribute.values,1)\r\n","\r\n","        ground_truth_missing = np.max(ground_truth_attribute.values,1) == np.min(ground_truth_attribute.values,1)\r\n","        clean_missing = np.max(cleaned_attribute.values,1) == np.min(cleaned_attribute.values,1)\r\n","        dirty_missing = np.max(dirty_attribute.values,1) == np.min(dirty_attribute.values,1)\r\n","\r\n","        # If we are working on data where no noise was added at all, then F1 and accuracy scores and such are not applicable\r\n","        # We can use TP and FN to pass upwards how many values were flipped\r\n","        # if verify_data.equals(test_data):\r\n","            # # Changed values & missing entries made non-missing\r\n","            # TP = np.count_nonzero((ground_truth_missing & ~clean_missing) | (  (~ground_truth_missing & ~clean_missing)  &    (ground_truth_val != clean_val)))\r\n","            # # Value to missing\r\n","            # FP=np.count_nonzero(~ground_truth_missing & clean_missing)\r\n","            # # Missing to missing\r\n","            # FN=np.count_nonzero(ground_truth_missing & clean_missing)\r\n","            # # Value stayed the same\r\n","            # TN=np.count_nonzero((~ground_truth_missing & ~clean_missing)  & (ground_truth_val==clean_val))\r\n","        # else:\r\n","        \r\n","        # Don't count instances where ground truth was missing, because we simply have no idea.\r\n","        # True positive: Was missing or wrong, now correct\r\n","        TP = np.count_nonzero( ~ground_truth_missing & ( ((ground_truth_val != dirty_val) | dirty_missing) & ~((ground_truth_val != clean_val) | clean_missing)) )\r\n","        # False positive: Was correct, now missing or wrong\r\n","        FP = np.count_nonzero( ~ground_truth_missing & ( ~((ground_truth_val != dirty_val) | dirty_missing) & ((ground_truth_val != clean_val) | clean_missing)) )\r\n","        # False negative: Was incorrect/missing and still is\r\n","        FN = np.count_nonzero( ~ground_truth_missing & ( ((ground_truth_val != dirty_val) | dirty_missing) & ((ground_truth_val != clean_val) | clean_missing)) )\r\n","        # True negative: was correct and stayed correct\r\n","        TN = np.count_nonzero( ~ground_truth_missing & ( ~((ground_truth_val != dirty_val) | dirty_missing) & ~((ground_truth_val != clean_val) | clean_missing)) )\r\n","\r\n","        flip_TP.append(TP)\r\n","        flip_FN.append(FN)\r\n","        flip_FP.append(FP)\r\n","        flip_TN.append(TN)\r\n","\r\n","        entropy_before_cleaning_per_row = scipy.stats.entropy(dirty_attribute,axis=1).sum()\r\n","        entropy_after_cleaning_per_row = scipy.stats.entropy(cleaned_attribute,axis=1).sum()\r\n","\r\n","        entropy_before.append(entropy_before_cleaning_per_row)\r\n","        entropy_after.append(entropy_after_cleaning_per_row)\r\n","        \r\n","        if bins is None:\r\n","            cleaned_database_non_pdb.iloc[:,column_index] = clean_val\r\n","        else:\r\n","            if np.issubdtype(bins[column_index].dtype,np.number):\r\n","                bin_width=bins[column_index][1]-bins[column_index][0]\r\n","                cleaned_database_non_pdb.iloc[:,column_index] = bins[column_index][clean_val] + 0.5*bin_width\r\n","            else:\r\n","                cleaned_database_non_pdb.iloc[:,column_index] = bins[column_index][clean_val]\r\n","\r\n","        i += size\r\n","    \r\n","    cleaned_database_non_pdb.to_csv(\"output_data/\" + full_string + \"/post_cleaning_non_pdb\" + gpu_string + \".csv\")\r\n","\r\n","    JSD_before = np.nansum(distances_before)\r\n","    JSD_after = np.nansum(distances_after)\r\n","\r\n","    flip_TP = np.nansum(flip_TP)\r\n","    flip_FN = np.nansum(flip_FN)\r\n","    flip_FP = np.nansum(flip_FP)\r\n","    flip_TN = np.nansum(flip_TN)\r\n","\r\n","    entropy_before = np.nansum(entropy_before)\r\n","    entropy_after = np.nansum(entropy_after)\r\n","\r\n","    # TODO sum entropy\r\n","\r\n","    # noise_left = 100 * avg_distance_after / avg_distance_before\r\n","    return JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after\r\n","\r\n","\r\n","def custom_loss(y_true, y_pred, sizes_sorted, loss_func):\r\n","    i = 0\r\n","    total_loss = 0\r\n","    if loss_func == 'JSD':\r\n","        loss_func = JSD\r\n","    elif loss_func == \"CCE\":\r\n","        loss_func = keras.losses.categorical_crossentropy\r\n","    else:\r\n","        loss_func = keras.losses.get(loss_func)\r\n","\r\n","    loss_list = []\r\n","\r\n","    for size in sizes_sorted:\r\n","        new_loss = loss_func(y_true[:, i:i + size], y_pred[:, i:i + size])\r\n","        loss_list.append(new_loss)\r\n","        i += size\r\n","    good_loss = tf.math.add_n(loss_list)\r\n","    return good_loss\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# -------------\r\n","def run_experiment(full_string=None,epochs=epochs_default, use_previous_df=False, BN_size=BN_size_default,\r\n","                   sampling_density=sampling_density_default, mu=mu_default, sigma=sigma_default,\r\n","                   activation_types=activation_types_default, hidden_layers=hidden_layers_default,\r\n","                   encoding_dim=encoding_dim_default, loss_function=loss_function_default,\r\n","                   training_method=training_method_default, activity_regularizer=activity_regularizer_default,\r\n","                   input_layer_type=input_layer_type_default, labeled_data_percentage=labeled_data_percentage_default,\r\n","                   VAE=VAE_default, CNN=CNN_default, kernel_landmarks=kernel_landmarks_default,\r\n","                   CNN_layers=CNN_layers_default, CNN_filters=CNN_filters_default,\r\n","                   CNN_kernel_size=CNN_kernel_size_default, gaussian_noise_sigma=gaussian_noise_sigma_default,\r\n","                   use_gaussian_noise=use_gaussian_noise_default, use_missing_entry=use_missing_entry_default,\r\n","                   missing_entry_prob=missing_entry_prob_default,rows=rows_default,use_file=use_file_default):\r\n","    if TEST_RUN_EPOCHS:\r\n","        epochs = TEST_RUN_EPOCH_NR\r\n","\r\n","    if use_file is None:\r\n","        bn = make_bn(BN_size, sampling_density)\r\n","        df, hard_evidence, sizes_sorted, gaussian_noise_layer_sigma_new,original_database,bins = make_df(use_file, bn, mu, sigma, use_gaussian_noise, use_missing_entry,\r\n","                                              missing_entry_prob,rows,full_string,sampling_density,gaussian_noise_sigma)\r\n","    else:\r\n","        df, hard_evidence, sizes_sorted, gaussian_noise_layer_sigma_new,original_database,bins = make_df(use_file, None, mu, sigma, use_gaussian_noise, use_missing_entry,\r\n","                                              missing_entry_prob,rows,full_string,sampling_density,gaussian_noise_sigma)\r\n","\r\n","    if loss_function != 'MSE':\r\n","        old_loss = loss_function[:]\r\n","        loss_function = lambda y_true, y_pred: custom_loss(y_true, y_pred, sizes_sorted, old_loss)\r\n","\r\n","    autoencoder = train_network(epochs, df, hard_evidence, activation_types, hidden_layers, encoding_dim, sizes_sorted,\r\n","                                loss_function, training_method, activity_regularizer, input_layer_type,\r\n","                                labeled_data_percentage, VAE, CNN, kernel_landmarks, CNN_layers, CNN_filters,\r\n","                                CNN_kernel_size, gaussian_noise_layer_sigma_new)\r\n","    JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after = measure_performance(df, hard_evidence, autoencoder, sizes_sorted,rows,full_string,original_database,bins)\r\n","\r\n","    autoencoder.save(\"output_data/\" + full_string + \"/model.h5\")\r\n","    del autoencoder\r\n","    gc.collect()\r\n","    keras.backend.clear_session()\r\n","    return JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after\r\n","\r\n","\r\n","clean_directory()\r\n","\r\n","runs = 0\r\n","lowest_results = 0\r\n","\r\n","while lowest_results < 10:\r\n","    lowest_results = min([len(x) for x in experiment_config_JSD_after])\r\n","    # lowest_results_forcpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\r\n","    #                              (x['config']['sampling_density'] * x['config']['BN_size']) < 100])\r\n","    # lowest_results_forgpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\r\n","    #                              (x['config']['sampling_density'] * x['config']['BN_size']) >= 100])\r\n","    highest_results = max([len(x) for x in experiment_config_JSD_after])\r\n","    print(\"\\n\\n----- LOWEST RESULTS: \" + str(lowest_results) + \", HIGHEST: \" + str(highest_results) + \" ------\\n\\n\")\r\n","    for i in (range(len(experiments))):\r\n","        experiment = experiments[i]\r\n","        x = experiment\r\n","        previous_runs = len(experiment_config_JSD_after[experiment['mapping']])\r\n","        if previous_runs == lowest_results:\r\n","            # if previous_runs==lowest_results_forcpu and (x['config']['sampling_density']*x['config']['BN_size'])<100:\r\n","            # if previous_runs==lowest_results_forgpu and (x['config']['sampling_density']*x['config']['BN_size'])>=100:\r\n","            # if runs == 0:\r\n","            #     print(i)\r\n","            if runs % 10 == 0 and runs > 0:\r\n","                clear_output(wait=True)\r\n","            JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after = run_experiment(experiment['full_string'],**experiment['config'])\r\n","\r\n","            if JSD_before > 0:\r\n","                JSD_reduction = 100 - ((JSD_after / JSD_before)*100)\r\n","            elif JSD_before == JSD_after:\r\n","                JSD_reduction = 0\r\n","            else:\r\n","                JSD_reduction = -np.inf\r\n","            accuracy = 100 * ((flip_TP+flip_TN) / (flip_TP+flip_TN+flip_FP+flip_FN))\r\n","            f1_score = 100 * ((flip_TP) / (flip_TP + 0.5*(flip_FP + flip_FN)))\r\n","            if entropy_before > 0:\r\n","                entropy_reduction = 100 - ((entropy_after / entropy_before)*100)\r\n","            elif entropy_before == entropy_after:\r\n","                entropy_reduction = 0\r\n","            else:\r\n","                entropy_reduction = -np.inf\r\n","            \r\n","            result_prints = pd.DataFrame([*experiment['full_string_list'],JSD_reduction,accuracy,f1_score,entropy_reduction]).T\r\n","            result_prints.columns = [\"Base config\",\"Parameter\",\"Value\",\"Noise reduction\",\"Accuracy\",\"F1 score\",\"Entropy reduction\"]\r\n","            result_prints.index=[runs]\r\n","            display(result_prints)\r\n","            # print(\"(\" + str(i) + \") \" + experiment['full_string'] + \";    \" + \"Q: \" + str(JSD_reduction) + \" ACC: \" + str(accuracy) + \" F1: \" + str(f1_score) + \" H_red: \" + str(entropy_reduction))\r\n","\r\n","            experiment_config_JSD_before[experiment['mapping']].append(JSD_before)\r\n","            experiment_config_JSD_after[experiment['mapping']].append(JSD_after)\r\n","\r\n","            experiment_config_flip_TP[experiment['mapping']].append(flip_TP)\r\n","            experiment_config_flip_TN[experiment['mapping']].append(flip_TN)\r\n","            experiment_config_flip_FP[experiment['mapping']].append(flip_FP)\r\n","            experiment_config_flip_FN[experiment['mapping']].append(flip_FN)\r\n","\r\n","            experiment_config_entropy_before[experiment['mapping']].append(entropy_before)\r\n","            experiment_config_entropy_after[experiment['mapping']].append(entropy_after)\r\n","\r\n","            #             experiment_configs_and_results[freeze(experiment['config'])].append(result)\r\n","\r\n","\r\n","            experiment_config_JSD_before_csv = [[experiment_config_strings[i]] + experiment_config_JSD_before[i] for i in range(len(experiment_config_JSD_before))]\r\n","            experiment_config_JSD_after_csv = [[experiment_config_strings[i]] + experiment_config_JSD_after[i] for i in range(len(experiment_config_JSD_after))]\r\n","\r\n","            experiment_config_flip_TP_csv = [[experiment_config_strings[i]] + experiment_config_flip_TP[i] for i in range(len(experiment_config_flip_TP))]\r\n","            experiment_config_flip_TN_csv = [[experiment_config_strings[i]] + experiment_config_flip_TN[i] for i in range(len(experiment_config_flip_TN))]\r\n","            experiment_config_flip_FP_csv = [[experiment_config_strings[i]] + experiment_config_flip_FP[i] for i in range(len(experiment_config_flip_FP))]\r\n","            experiment_config_flip_FN_csv = [[experiment_config_strings[i]] + experiment_config_flip_FN[i] for i in range(len(experiment_config_flip_FN))]\r\n","\r\n","            experiment_config_entropy_before_csv = [[experiment_config_strings[i]] + experiment_config_entropy_before[i] for i in range(len(experiment_config_entropy_before))]\r\n","            experiment_config_entropy_after_csv = [[experiment_config_strings[i]] + experiment_config_entropy_after[i] for i in range(len(experiment_config_entropy_after))]\r\n","\r\n","            with DelayedKeyboardInterrupt():\r\n","                if not os.path.exists(\"results/\"):\r\n","                    os.makedirs(\"results/\")\r\n","                with open(\"results/experiment_config_JSD_before\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_JSD_before_csv)\r\n","                with open(\"results/experiment_config_JSD_after\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_JSD_after_csv)\r\n","                with open(\"results/experiment_config_flip_TP\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_TP_csv)\r\n","                with open(\"results/experiment_config_flip_TN\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_TN_csv)\r\n","                with open(\"results/experiment_config_flip_FP\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_FP_csv)\r\n","                with open(\"results/experiment_config_flip_FN\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_FN_csv)\r\n","                with open(\"results/experiment_config_entropy_before\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_entropy_before_csv)\r\n","                with open(\"results/experiment_config_entropy_after\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_entropy_after_csv)\r\n","                with open(\"output_data/experiments\" + gpu_string, \"wb\") as dill_file:\r\n","                    dill.dump(experiments, dill_file)\r\n","\r\n","            runs += 1\r\n","    lowest_results = min([len(x) for x in experiment_config_JSD_after])\r\n","    # lowest_results_forcpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\r\n","    #                              (x['config']['sampling_density'] * x['config']['BN_size']) < 100])\r\n","    # lowest_results_forgpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\r\n","    #                              (x['config']['sampling_density'] * x['config']['BN_size']) >= 100])\r\n","\r\n","\r\n","\r\n","clean_directory()\r\n","\r\n","\r\n"],"outputs":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}