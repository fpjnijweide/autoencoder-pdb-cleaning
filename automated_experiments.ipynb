{"cells":[{"cell_type":"markdown","source":["   # Automated experiments\n","\n","   This file runs all the experiments described in the paper. It will keep running until there are n=10 measurements for every configuration, which might take a few days."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Settings\r\n","\r\n","USE_GPU = False # Turn on to use GPU and to append \"_gpu\" behind all saved files\r\n","\r\n","TEST_RUN_EPOCHS = False  # Whether to force the number of epochs below. Useful to test for errors without having to wait for hours of training\r\n","TEST_RUN_EPOCH_NR = 1\r\n","LOAD_DATA = True  # Whether to add results to previously saved .csv data\r\n","verbosity = 0\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import os\r\n","\r\n","if USE_GPU:\r\n","    gpu_string = \"_gpu\"\r\n","else:\r\n","    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n","    gpu_string = \"\"\r\n","\r\n","try:\r\n","    from IPython.display import display\r\n","    get_ipython().run_line_magic('matplotlib', 'inline')\r\n","except:\r\n","    display = print\r\n","\r\n","import csv\r\n","import tensorflow as tf\r\n","import scipy.stats\r\n","import scipy.spatial\r\n","import numpy as np\r\n","import tensorflow.keras as keras\r\n","import pyAgrum as gum\r\n","import pandas as pd\r\n","import sklearn.model_selection\r\n","import math\r\n","import gc\r\n","import gkernel  # gkernel.py in this folder\r\n","import csv_to_pdb #csv_to_pdb.py in this folder\r\n","import dill\r\n","import signal\r\n","import logging\r\n","from tensorflow.python.framework import ops\r\n","from tensorflow.python.ops import math_ops\r\n","\r\n","from IPython.display import clear_output\r\n","\r\n","print('Imports done')\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from tensorflow.python.client import device_lib\r\n","\r\n","print(device_lib.list_local_devices())\r\n","print(tf.__version__)\r\n","print(\"Num GPUs Available: \" + str(len(tf.config.experimental.list_physical_devices('GPU'))))\r\n","tf.test.is_gpu_available()\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# # Default Variables for experiments.You probably want to use the cell above to change experiment vars.\r\n","BN_size_default = 3  # amount of BN variables, minimum 3\r\n","\r\n","mu_default = 0\r\n","sigma_default = 0.02  # Distribution of the noise\r\n","gaussian_noise_sigma_default = lambda SD: (0.01 / SD) * 100\r\n","sampling_density_default = 4  # How many bins the quasi-continuous variables use for distributing their probabilities. Higher_default = better approximation of continuous distributions\r\n","\r\n","activation_types_default = [keras.backend.sin, keras.backend.cos, keras.activations.linear, 'relu',\r\n","                            'swish']  # Activation layer types\r\n","hidden_layers_default = 3  # amount of hidden layers\r\n","encoding_dim_default = BN_size_default  # The dimensionality of the middle layer\r\n","loss_function_default = 'JSD'\r\n","training_method_default = 'semi'\r\n","\r\n","activity_regularizer_default = keras.regularizers.l2(10 ** -4)\r\n","activity_regularizer_default.__name__ = \"L2: 10^-4\"\r\n","\r\n","input_layer_type_default = 'gaussian_noise'\r\n","labeled_data_percentage_default = 2\r\n","\r\n","epochs_default = 100\r\n","VAE_default = False\r\n","CNN_default = False\r\n","kernel_landmarks_default = 100\r\n","\r\n","CNN_layers_default = 1\r\n","CNN_filters_default = 64\r\n","CNN_kernel_size_default = 3\r\n","\r\n","use_gaussian_noise_default = True\r\n","use_missing_entry_default = False\r\n","missing_entry_prob_default = 0.01\r\n","rows_default=10000\r\n","use_file_default=None\r\n","\r\n","defaults = dict(BN_size=BN_size_default, mu=mu_default, sigma=sigma_default, sampling_density=sampling_density_default,\r\n","                gaussian_noise_sigma=gaussian_noise_sigma_default, activation_types=activation_types_default,\r\n","                hidden_layers=hidden_layers_default, encoding_dim=encoding_dim_default,\r\n","                loss_function=loss_function_default, training_method=training_method_default,\r\n","                activity_regularizer=activity_regularizer_default, input_layer_type=input_layer_type_default,\r\n","                labeled_data_percentage=labeled_data_percentage_default, epochs=epochs_default, VAE=VAE_default,\r\n","                CNN=CNN_default, kernel_landmarks=kernel_landmarks_default, CNN_layers=CNN_layers_default,\r\n","                CNN_filters=CNN_filters_default, CNN_kernel_size=CNN_kernel_size_default,\r\n","                use_gaussian_noise=use_gaussian_noise_default, use_missing_entry=use_missing_entry_default,\r\n","                missing_entry_prob=missing_entry_prob_default,rows=rows_default,use_file=use_file_default)\r\n","\r\n","parameters = list(defaults.keys())\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["experiments = []\r\n","experiment_config_list = []\r\n","experiment_config_strings = []\r\n","\r\n","\r\n","experiment_config_JSD_before = []\r\n","experiment_config_JSD_after = []\r\n","\r\n","# noise reduction performance score: (noise after) / (noise before) in %\r\n","\r\n","experiment_config_flip_TP = [] # good flip\r\n","experiment_config_flip_TN = [] # good left unflipped\r\n","experiment_config_flip_FP = [] # unneeded flip\r\n","experiment_config_flip_FN = [] # wrong flip or unflipped\r\n","\r\n","# wrong attributes before = TP + FN\r\n","# correct attributes before = FP + TN\r\n","# wrong attributes after = FP + FN\r\n","# correct attributes after = TP + TN\r\n","\r\n","# REDUCTION IN WRONG VARIABLES = (FP + FN) / (TP + FN)\r\n","\r\n","# accuracy = TP + TN / (TP+TN+FP+FN)\r\n","# f1 score = (2TP) + (2TP + FP + FN)\r\n","\r\n","experiment_config_entropy_before = []\r\n","experiment_config_entropy_after = []\r\n","\r\n","# allows us to see whether total uncertainty increases or decreases\r\n","# entropy reduction score: H_before / H_after\r\n","\r\n","# we can then see if these correlate: is there an increase/decrease in performance when entropy is increased?\r\n","# how about f1/acc?\r\n","\r\n","\r\n","\r\n","class DelayedKeyboardInterrupt:\r\n","    # Original author: Gary van der Merwe at https://stackoverflow.com/a/21919644\r\n","    def __enter__(self):\r\n","        self.signal_received = False\r\n","        self.old_handler = signal.signal(signal.SIGINT, self.handler)\r\n","                \r\n","    def handler(self, sig, frame):\r\n","        self.signal_received = (sig, frame)\r\n","        logging.debug('SIGINT received. Delaying KeyboardInterrupt.')\r\n","    \r\n","    def __exit__(self, type, value, traceback):\r\n","        signal.signal(signal.SIGINT, self.old_handler)\r\n","        if self.signal_received:\r\n","            self.old_handler(*self.signal_received)\r\n","\r\n","def load_from_csv(input_string):\r\n","    with open(input_string, 'r') as fp:\r\n","        reader = csv.reader(fp)\r\n","        li = list(reader)\r\n","    newlist = []\r\n","    for row in li:\r\n","        newrow = []\r\n","        for entry in row[1:]:\r\n","            if entry == '':\r\n","                break\r\n","            else:\r\n","                newrow.append(float(entry))\r\n","        newlist.append(newrow)\r\n","    return newlist\r\n","\r\n","\r\n","def find(inputlist, search, key=lambda z: z):\r\n","    for i in range(len(inputlist)):\r\n","        if key(inputlist[i]) == search:\r\n","            return i\r\n","    return None\r\n","\r\n","\r\n","def str_noneguard(obj):\r\n","    if hasattr(obj, '__name__'):\r\n","        return obj.__name__\r\n","    if obj is None:\r\n","        return ''\r\n","    if isinstance(obj, list):\r\n","        return str([str_noneguard(x) for x in obj])\r\n","    return str(obj)\r\n","\r\n","\r\n","def freeze(d):\r\n","    # thanks to https://stackoverflow.com/a/13264725\r\n","    if isinstance(d, dict):\r\n","        return frozenset((key, freeze(value)) for key, value in d.items())\r\n","    elif isinstance(d, list):\r\n","        return tuple(freeze(value) for value in d)\r\n","    return d\r\n","\r\n","\r\n","def gen_experiment(config_string, input_dict={}, parameter=None, vars=None):\r\n","    if parameter is None:\r\n","        vars = [None]\r\n","\r\n","    for x in vars:\r\n","        if (not parameter=='activity_regularizer') and (x is None or x == 'default') and (not parameter is None):\r\n","            x = defaults[parameter]\r\n","\r\n","        new_experiment_config = input_dict.copy()\r\n","        if parameter == 'input_layer_type' and x == 'VAE':\r\n","            new_experiment_config['VAE'] = True\r\n","        elif parameter == 'input_layer_type' and x == 'CNN':\r\n","            new_experiment_config['CNN'] = True\r\n","        elif parameter == 'missing_entry':\r\n","            new_experiment_config['use_missing_entry'] = True\r\n","            new_experiment_config['use_gaussian_noise'] = False\r\n","            new_experiment_config['missing_entry_prob'] = x\r\n","        elif parameter == 'missing_entry_combined':\r\n","            new_experiment_config['use_missing_entry'] = True\r\n","            new_experiment_config['use_gaussian_noise'] = True\r\n","            new_experiment_config['missing_entry_prob'] = x\r\n","        elif parameter == 'missing_entry_no_denoising':\r\n","            new_experiment_config['use_missing_entry'] = True\r\n","            new_experiment_config['use_gaussian_noise'] = False\r\n","            new_experiment_config['input_layer_type'] = 'dense'\r\n","            new_experiment_config['missing_entry_prob'] = x\r\n","        elif parameter == 'kernel_landmarks':\r\n","            new_experiment_config['input_layer_type'] = 'gaussian_kernel'\r\n","            new_experiment_config[parameter] = x\r\n","        elif parameter == 'CNN_kernel_size' or parameter == 'CNN_filters':\r\n","            new_experiment_config['CNN'] = True\r\n","            new_experiment_config[parameter] = x\r\n","        elif parameter == 'gaussian_noise_sigma':\r\n","            new_experiment_config['input_layer_type'] = 'gaussian_noise'\r\n","            new_experiment_config[parameter] = x\r\n","        elif parameter is not None:\r\n","            new_experiment_config[parameter] = x\r\n","\r\n","        full_string = str(config_string + \"    \" + str_noneguard(parameter) + \"    \" + str_noneguard(x))\r\n","        full_string_list = (config_string,str_noneguard(parameter),str_noneguard(x))\r\n","\r\n","        if new_experiment_config in experiment_config_list:\r\n","            mapping = experiment_config_list.index(new_experiment_config)\r\n","        else:\r\n","            mapping = len(experiment_config_list)\r\n","            experiment_config_list.append(new_experiment_config)\r\n","            experiment_config_strings.append(full_string)\r\n","            \r\n","            experiment_config_JSD_before.append([])\r\n","            experiment_config_JSD_after.append([])\r\n","\r\n","            experiment_config_flip_TP.append([])\r\n","            experiment_config_flip_TN.append([])\r\n","            experiment_config_flip_FP.append([])\r\n","            experiment_config_flip_FN.append([])\r\n","\r\n","            experiment_config_entropy_before.append([])\r\n","            experiment_config_entropy_after.append([])\r\n","\r\n","        experiments.append(\r\n","            {'config_string': config_string, 'input_dict': input_dict, 'parameter': parameter, 'vars': vars,\r\n","             'current_var': x, 'config': new_experiment_config, 'full_string': full_string, 'mapping': mapping, 'full_string_list':full_string_list})\r\n","\r\n","\r\n","ground_config_strings = [\"CCE, SD=4\", \"JSD, SD=4\", \"CCEu, SD=4\", \"JSDu, SD=4\", \"CCE, SD=100\", \"JSD, SD=100\",\r\n","                         \"CCEu, SD=100\", \"JSDu, SD=100\"]\r\n","# ground_config_strings = [\"JSD, surgical_case_durations\",\"JSDu, surgical_case_durations\",\"JSD, LBP RA\",\"JSDu, LBP RA\"]\r\n","\r\n","for config_string in ground_config_strings:\r\n","    ground_config = defaults.copy()\r\n","    if \"CCE\" in config_string:\r\n","        ground_config['loss_function'] = 'CCE'\r\n","    elif \"JSD\" in config_string:\r\n","        ground_config['loss_function'] = 'JSD'\r\n","    elif \"MSE\" in config_string:\r\n","        ground_config['loss_function'] = 'MSE'\r\n","    elif \"KLD\" in config_string:\r\n","        ground_config['loss_function'] = 'KLD'\r\n","\r\n","    if \"u,\" in config_string:\r\n","        ground_config['training_method'] = 'unsupervised'\r\n","    if \"SD=100\" in config_string:\r\n","        ground_config['sampling_density'] = 100\r\n","    if \"surgical_case_durations\" in config_string:\r\n","        ground_config['use_file']=\"surgical_case_durations.csv\"\r\n","    if \"LBP RA\" in config_string:\r\n","        ground_config['use_file']=\"Dataset - LBP RA.csv\"\r\n","\r\n","    # sigma_list = [0, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\r\n","    # gen_experiment(config_string, ground_config, 'sigma', sigma_list)\r\n","    # gen_experiment(config_string, ground_config, 'missing_entry', [0,0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","    # gen_experiment(config_string, ground_config, 'missing_entry_combined', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","    # 0\r\n","    if ground_config['training_method'] != 'unsupervised':\r\n","        gen_experiment(config_string, ground_config, 'training_method',\r\n","                       [\"supervised\", \"supervised_2_percent\", \"semi\", \"semi_sup_first\", \"semi_mixed\", \"unsupervised\"])\r\n","\r\n","    if ground_config['loss_function'] != 'CCE':\r\n","        # 1\r\n","        activation_list = [defaults['activation_types'], ['relu'], ['relu'] * 5,\r\n","                           [keras.backend.sin, keras.backend.cos, keras.activations.linear],\r\n","                           [keras.backend.sin, keras.backend.cos, keras.activations.linear, 'relu', 'sigmoid']]\r\n","        gen_experiment(config_string, ground_config, 'activation_types', activation_list)\r\n","\r\n","        # 2\r\n","        gen_experiment(config_string, ground_config, 'input_layer_type',\r\n","                       ['dense', 'gaussian_noise', 'gaussian_dropout', 'sqrt_softmax', 'gaussian_kernel', 'CNN', 'VAE'])\r\n","\r\n","        # 3\r\n","        gen_experiment(config_string, ground_config, 'encoding_dim', [2, 3, 6])\r\n","\r\n","        # 4\r\n","        gen_experiment(config_string, ground_config, 'hidden_layers', [3, 5, 7, 9, 27])\r\n","\r\n","        # 5\r\n","        regularizer_list = [None, keras.regularizers.l2(0.01), activity_regularizer_default,\r\n","                            keras.regularizers.l1(0.01), keras.regularizers.l1(10 ** -4)]\r\n","        regularizer_strings = [\"none\", \"L2: 0.01\", \"L2: 10^-4\", \"L1: 0.01\", \"L1: 10^-4\"]\r\n","        for i in range(len(regularizer_list)):\r\n","            try:\r\n","                regularizer_list[i].__name__ = regularizer_strings[i]\r\n","            except:\r\n","                pass\r\n","        gen_experiment(config_string, ground_config, 'activity_regularizer', regularizer_list)\r\n","\r\n","        sigma_list = [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\r\n","        gen_experiment(config_string, ground_config, 'sigma', sigma_list)\r\n","\r\n","        # 7\r\n","        if ground_config['sampling_density'] == 100:\r\n","            gen_experiment(config_string, ground_config, 'BN_size', [2, 3, 4, 5])\r\n","        else:\r\n","            gen_experiment(config_string, ground_config, 'BN_size', [2, 3, 4, 5, 10, 20, 30])\r\n","\r\n","    # 8\r\n","    if ground_config['training_method'] != 'unsupervised':\r\n","        gen_experiment(config_string, ground_config, 'labeled_data_percentage',\r\n","                       [99, 50, 20, 10, 5, 2, 1, 0.5, 0.25, 0.125, 0.05, 0.01])\r\n","    # 9\r\n","    if ground_config['sampling_density'] != 100:\r\n","        gen_experiment(config_string, ground_config, 'sampling_density', [4, 15, 25, 50, 100, 150, 300])\r\n","\r\n","    if ground_config['loss_function'] != 'CCE':\r\n","        # 10-13\r\n","        gaussian_noise_sigma_strings = [\"lambda SD: 0.01\", \"lambda SD: 0.02\", \"lambda SD: 0.05\", \"lambda SD: 0.1\",\r\n","                                        \"lambda SD: 0.2\", \"lambda SD: (0.01/SD)*100\", \"lambda SD: (0.02/SD)*100\",\r\n","                                        \"lambda SD: (0.05/SD)*100\", \"lambda SD: (0.1/SD)*100\",\r\n","                                        \"lambda SD: (0.2/SD)*100\"]\r\n","        gaussian_noise_sigma_list = [lambda SD: 0.01, lambda SD: 0.02, lambda SD: 0.05, lambda SD: 0.1, lambda SD: 0.2,\r\n","                                     lambda SD: (0.01 / SD) * 100, lambda SD: (0.02 / SD) * 100,\r\n","                                     lambda SD: (0.05 / SD) * 100, lambda SD: (0.1 / SD) * 100,\r\n","                                     lambda SD: (0.2 / SD) * 100]\r\n","        for i in range(len(gaussian_noise_sigma_list)):\r\n","            gaussian_noise_sigma_list[i].__name__ = gaussian_noise_sigma_strings[i]\r\n","        gen_experiment(config_string, ground_config, 'gaussian_noise_sigma', gaussian_noise_sigma_list)\r\n","\r\n","        gen_experiment(config_string, ground_config, 'missing_entry', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","        gen_experiment(config_string, ground_config, 'missing_entry_combined', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","        gen_experiment(config_string, ground_config, 'missing_entry_no_denoising', [0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\r\n","        #14\r\n","        gen_experiment(config_string, ground_config, 'rows', [10**2,10**3,10**4,10**5,10**6])\r\n","\r\n","\r\n","if LOAD_DATA:\r\n","    try:\r\n","        experiment_config_JSD_before = load_from_csv(\"results/experiment_config_JSD_before\" + gpu_string + \".csv\")\r\n","        experiment_config_JSD_after = load_from_csv(\"results/experiment_config_JSD_after\" + gpu_string + \".csv\")\r\n","\r\n","        experiment_config_flip_TP = load_from_csv(\"results/experiment_config_flip_TP\" + gpu_string + \".csv\")\r\n","        experiment_config_flip_TN = load_from_csv(\"results/experiment_config_flip_TN\" + gpu_string + \".csv\")\r\n","        experiment_config_flip_FP = load_from_csv(\"results/experiment_config_flip_FP\" + gpu_string + \".csv\")\r\n","        experiment_config_flip_FN = load_from_csv(\"results/experiment_config_flip_FN\" + gpu_string + \".csv\")\r\n","\r\n","        experiment_config_entropy_before = load_from_csv(\"results/experiment_config_entropy_before\" + gpu_string + \".csv\")\r\n","        experiment_config_entropy_after = load_from_csv(\"results/experiment_config_entropy_after\" + gpu_string + \".csv\")\r\n","    except:\r\n","        print('could not load data')\r\n","\r\n","print(\"Experiments: \" + str(len(experiments)))\r\n","print(\"Experiment configs: \" + str(len(experiment_config_list)))\r\n","print(\"\\n\\n\\n----------DONE---------\\n\\n\\n\")\r\n","\r\n","with pd.option_context(\"display.max_rows\", 1000):\r\n","    display(pd.DataFrame(pd.DataFrame(experiments).loc[:,\"full_string_list\"].values.tolist()))\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# @dispatch.add_dispatch_support\n","def JSD(y_true, y_pred):\n","    #     y_pred = ops.convert_to_tensor_v2(y_pred)\n","    #     y_true = math_ops.cast(y_true, y_pred.dtype)\n","    #     y_true = keras.backend.clip(y_true, keras.backend.epsilon(), 1)\n","    #     y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1)\n","    y_pred = ops.convert_to_tensor_v2(y_pred)\n","    y_true = math_ops.cast(y_true, y_pred.dtype)\n","    y_true = keras.backend.clip(y_true, keras.backend.epsilon(), 1)\n","    y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1)\n","    means = 0.5 * (y_true + y_pred)\n","    divergence_tensor = 0.5 * keras.losses.kld(y_true, means) + 0.5 * keras.losses.kld(y_pred, means)\n","\n","    return divergence_tensor\n","\n","\n","def generate_samplespace(func, x_min, x_max, sampling_density):\n","    grid = np.linspace(x_min, x_max, sampling_density + 1)\n","    probs = np.diff(func.cdf(grid))\n","    return probs / np.sum(probs)  # ensuring it is normalized\n","\n","\n","def normalize_df(df):\n","    newdf = df.div(df.sum(axis=1), axis=0)\n","    SD = len(newdf.columns)\n","    return newdf.fillna(1 / SD)\n","\n","\n","def prob_distance(x, y):\n","    xt = x.T  # this is bad practice, but I cannot debug the statements below without doing this\n","    yt = y.T\n","    res = JSD(x, y)\n","    return res\n","\n","\n","def make_bn(BN_size, sampling_density):\n","    bn = gum.BayesNet(\"Quasi-Continuous\")\n","    a = bn.add(gum.LabelizedVariable(\"A\", \"A binary variable\", 2))\n","    bn.cpt(a)[:] = [0.4, 0.6]\n","\n","    if BN_size > 1:\n","        b = bn.add(gum.RangeVariable(\"B\", \"A range variable\", 0, sampling_density - 1))\n","        bn.addArc(a, b)\n","        first = generate_samplespace(scipy.stats.truncnorm(-10, 3), -10, 3, sampling_density)\n","        second = generate_samplespace(scipy.stats.truncnorm(-2, 6), -2, 6, sampling_density)\n","        bn.cpt(b)[{'A': 0}] = first\n","        bn.cpt(b)[{'A': 1}] = second\n","\n","    if BN_size > 2:\n","        c = bn.add(gum.RangeVariable(\"C\", \"Another quasi continuous variable\", 0, sampling_density - 1))\n","        bn.addArc(b, c)\n","        l = []\n","        for i in range(sampling_density):\n","            # the size and the parameter of gamma depends on the parent value\n","            k = (i * 30.0) / sampling_density\n","            l.append(generate_samplespace(scipy.stats.gamma(k + 1), 4, 5 + k, sampling_density))\n","        bn.cpt(c)[:] = l\n","\n","        for d in range(BN_size - 3):\n","            # new variable\n","            d = bn.add(gum.RangeVariable(\"D\" + str(d), \"Another quasi continuous variable\", 0, sampling_density - 1))\n","            l = []\n","            bn.addArc(c, d)\n","            for i in range(sampling_density):\n","                # the size and the parameter of gamma depends on the parent value\n","                k = (i * 30.0) / sampling_density\n","                l.append(generate_samplespace(scipy.stats.gamma(k + 1), 4, 5 + k, sampling_density))\n","            bn.cpt(d)[:] = l\n","\n","    return bn\n","\n","\n","def make_df(use_file, bn, mu, sigma, use_gaussian_noise, use_missing_entry, missing_entry_prob,rows,full_string,sampling_density,gaussian_noise_layer_sigma):\n","    if not os.path.exists(\"databases/\" + full_string+\"/\"):\n","        os.makedirs(\"databases/\" + full_string+\"/\")\n","\n","    if use_file is not None:\n","        original_database,sizes_sorted,hard_evidence = csv_to_pdb.make_pdb(use_file)\n","    else:\n","        gum.generateCSV(bn, \"databases/\" + full_string + \"/database_original\" + gpu_string + \".csv\", rows)\n","        original_database = pd.read_csv(\"databases/\" + full_string + \"/database_original\" + gpu_string + \".csv\")\n","        original_database = original_database.reindex(sorted(original_database.columns), axis=1)\n","    original_database.to_csv(\"databases/\" + full_string + \"/database_original\" + gpu_string + \".csv\")\n","\n","    if use_file is not None:\n","        pass\n","    else:\n","        size_dict = {}\n","        for column_name in original_database.columns:\n","            size_dict[column_name] = bn.variable(column_name).domainSize()\n","\n","        shape = [original_database.shape[0], sum(size_dict.values())]\n","\n","        df_cols_sorted = sorted(list(original_database.columns))\n","        sizes_sorted = [size_dict[x] for x in df_cols_sorted]\n","        sizes_sorted_with_leading_zero = [0] + sizes_sorted\n","\n","        data = np.ones(original_database.shape[0] * original_database.shape[1])\n","        row = list(range(original_database.shape[0])) * original_database.shape[1]\n","        col = []\n","        for i in range(original_database.values.T.shape[0]):\n","            for item in original_database.values.T[i]:\n","                col.append(item + sum(sizes_sorted_with_leading_zero[0:i + 1]))\n","\n","        input3 = scipy.sparse.coo_matrix((data, (row, col)), shape=tuple(shape)).todense()\n","\n","        first_id2 = df_cols_sorted[:]\n","        second_id2 = [list(range(x)) for x in sizes_sorted]\n","\n","        arrays3 = [np.repeat(first_id2, sizes_sorted), [item for sublist in second_id2 for item in sublist]]\n","        tuples2 = list(zip(*arrays3))\n","        index2 = pd.MultiIndex.from_tuples(tuples2, names=['Variable', 'Value'])\n","\n","        hard_evidence = pd.DataFrame(input3, columns=index2)\n","    hard_evidence.to_csv(\"databases/\" + full_string + \"/ground_truth\" + gpu_string + \".csv\")\n","\n","    df = hard_evidence + 0\n","\n","    \n","    # TODO rework how sigma is generated here, make it per column\n","    sigma = (sigma / sampling_density) * 100\n","    # TODO rework how gaussian noise layer sigma is generated here, make it a list of sigmas (generated in the same way as the sigmas before)\n","    gaussian_noise_layer_sigma_new = gaussian_noise_layer_sigma(sampling_density)\n","    noise = np.random.normal(mu, sigma, hard_evidence.shape)\n","    if use_gaussian_noise:\n","        df = df + noise\n","        df = df.clip(lower=0, upper=1)\n","    if use_missing_entry:\n","        amount_of_variables = len(sizes_sorted)\n","        rows = len(df)\n","        total_entries = amount_of_variables * rows  # amount of probability distributions in the PDB\n","        missing_entry_nrs = np.random.choice(total_entries, size=round(total_entries * missing_entry_prob),\n","                                             replace=False)\n","        m = missing_entry_nrs[:]  # using an alias for shorter code\n","        col_index = 0\n","        for attribute_nr, size in enumerate(sizes_sorted):\n","            entries_this_col = m[(m >= rows * attribute_nr) & (m < rows * (attribute_nr + 1))]\n","            rows_this_col = entries_this_col - (rows * attribute_nr)\n","            df.iloc[rows_this_col, col_index:col_index + size] = 1\n","\n","            col_index += size\n","\n","    for col in df_cols_sorted:\n","        df[col] = normalize_df(df[col])\n","\n","    df.to_csv(\"databases/\" + full_string + \"/noisy_data\" + gpu_string + \".csv\")\n","\n","    return df, hard_evidence, sizes_sorted,gaussian_noise_layer_sigma_new,original_database\n","\n","\n","class Sampling(keras.layers.Layer):\n","    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n","\n","    def call(self, inputs):\n","        z_mean, z_log_var = inputs\n","        batch = tf.shape(z_mean)[0]\n","        dim = tf.shape(z_mean)[1]\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n","\n","\n","class VAE_model(keras.Model):\n","    def __init__(self, encoder, decoder, loss_func, **kwargs):\n","        super(VAE_model, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.loss_func = loss_func\n","        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n","        self.reconstruction_loss_tracker = keras.metrics.Mean(\n","            name=\"reconstruction_loss\"\n","        )\n","        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [\n","            self.total_loss_tracker,\n","            self.reconstruction_loss_tracker,\n","            self.kl_loss_tracker,\n","        ]\n","\n","    def call(self, data):\n","        z_mean, z_log_var, z = self.encoder(data)\n","        reconstruction = self.decoder(z)\n","        return reconstruction\n","\n","    def train_step(self, data):\n","        with tf.GradientTape() as tape:\n","            x, y = data\n","            z_mean, z_log_var, z = self.encoder(x)\n","            reconstruction = self.decoder(z)\n","\n","            #             reconstruction_loss = self.loss_func(data, reconstruction)\n","\n","            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","\n","            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","\n","            reconstruction_loss = tf.reduce_mean(\n","                self.loss_func(y, reconstruction)\n","            )\n","\n","            total_loss = reconstruction_loss + kl_loss\n","        grads = tape.gradient(total_loss, self.trainable_weights)\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","        return {\n","            \"loss\": self.total_loss_tracker.result(),\n","            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n","            \"kl_loss\": self.kl_loss_tracker.result(),\n","        }\n","\n","\n","def train_network(epochs, df, hard_evidence, activation_types, hidden_layers, encoding_dim, sizes_sorted, loss_function,\n","                  training_method, activity_regularizer, input_layer_type, labeled_data_percentage, VAE, CNN,\n","                  kernel_landmarks, CNN_layers, CNN_filters, CNN_kernel_size, gaussian_noise_sigma):             \n","    x_train, y_train, x_train_nolabel = None, None, None\n","\n","    if training_method == 'supervised':\n","        x_train, y_train = df, hard_evidence\n","    elif training_method == \"unsupervised\":\n","        x_train = df\n","    elif training_method == \"supervised_2_percent\":\n","        x_train, _, y_train, _ = sklearn.model_selection.train_test_split(df, hard_evidence, test_size=0.98)\n","    elif training_method == \"semi\" or training_method == \"semi_supervised\" or training_method == \"semisupervised\" or training_method == \"semi_sup_first\" or training_method == \"semi_mixed\":\n","        x_train, x_train_nolabel, y_train, _ = sklearn.model_selection.train_test_split(df, hard_evidence, test_size=(\n","                                                                                                                             100 - labeled_data_percentage) / 100)  # semi supervised\n","    else:\n","        raise Exception(\"Invalid training method\")\n","\n","    x_train = np.float32(x_train)\n","    if y_train is not None:\n","        y_train = np.float32(y_train)\n","    if x_train_nolabel is not None:\n","        x_train_nolabel = np.float32(x_train_nolabel)\n","\n","    # types = ['relu','relu','relu','relu','relu']\n","    input_dim = sum(sizes_sorted)\n","    # this is our input placeholder\n","    #     input_layer = keras.layers.Input(shape=(None,input_dim))\n","    input_layer = keras.layers.Input(shape=(input_dim,))\n","\n","    # \"encoded\" is the encoded representation of the input\n","    if input_layer_type == 'dense' and not CNN:\n","        x = keras.layers.Dense(input_dim, activation='relu', activity_regularizer=activity_regularizer)(input_layer)\n","    elif CNN:\n","        x = tf.expand_dims(input_layer, axis=2)\n","        x = keras.layers.Conv1D(input_shape=(0, input_dim), filters=CNN_filters, kernel_size=CNN_kernel_size,\n","                                activation='relu')(x)\n","        x = keras.layers.MaxPooling1D(pool_size=2)(x)\n","        x = keras.layers.Flatten()(x)\n","\n","    elif input_layer_type == 'gaussian_noise':\n","        x = keras.layers.GaussianNoise(gaussian_noise_sigma)(input_layer)\n","    elif input_layer_type == 'gaussian_dropout':\n","        x = keras.layers.GaussianDropout(0.01)(input_layer)\n","    elif input_layer_type == 'sqrt_softmax':\n","        x = keras.layers.Lambda(keras.backend.sqrt)(input_layer)\n","        x = keras.layers.Softmax()(x)\n","    elif input_layer_type == \"gaussian_kernel\":\n","\n","        x = gkernel.GaussianKernel3(kernel_landmarks, input_dim)(input_layer)\n","\n","    encode_ratio = 0.1\n","    middle = hidden_layers // 2\n","    for i in range(hidden_layers):\n","        if CNN and i < CNN_layers - 1:\n","\n","            x = tf.expand_dims(x, axis=2)\n","            x = keras.layers.Conv1D(filters=CNN_filters, kernel_size=CNN_kernel_size, activation='relu')(x)\n","            x = keras.layers.MaxPooling1D(pool_size=2)(x)\n","            x = keras.layers.Flatten()(x)\n","\n","        elif VAE and i == middle:\n","\n","            z_mean = keras.layers.Dense(encoding_dim, name=\"z_mean\")(x)\n","            z_log_var = keras.layers.Dense(encoding_dim, name=\"z_log_var\")(x)\n","            z = Sampling()([z_mean, z_log_var])\n","            latent_inputs = keras.layers.Input(shape=(encoding_dim,))\n","            x = tf.add(latent_inputs, 0)\n","\n","        else:\n","            ratio = 2 ** (math.log2(encode_ratio) + abs(i - middle))\n","            size = encoding_dim if i == middle else max((min(input_dim, int(ratio * input_dim))), encoding_dim)\n","            #         print(size)\n","            if len(activation_types) > 1:\n","                x = keras.layers.concatenate(\n","                    [keras.layers.Dense(size, activation=type, activity_regularizer=activity_regularizer)(x) for type in\n","                     activation_types], axis=1)\n","            else:\n","                x = keras.layers.Dense(size, activation=activation_types[0], activity_regularizer=activity_regularizer)(\n","                    x)\n","\n","    final_layer_list = [keras.layers.Dense(size, activation='softmax', activity_regularizer=activity_regularizer)(x) for\n","                        size in sizes_sorted]\n","\n","    decoded = keras.layers.concatenate(final_layer_list, axis=1)\n","\n","\n","    if VAE:\n","        encoder = keras.models.Model(input_layer, [z_mean, z_log_var, z], name=\"encoder\")\n","        decoder = keras.models.Model(latent_inputs, decoded, name=\"decoder\")\n","        autoencoder = VAE_model(encoder, decoder, loss_function)\n","        autoencoder.compile(optimizer='adam', metrics=['accuracy'])  # semi supervised\n","    else:\n","        autoencoder = keras.models.Model(input_layer, outputs=decoded)\n","        autoencoder.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])  # semi supervised\n","\n","    hist = keras.callbacks.History()\n","\n","    if training_method == 'supervised' or training_method == \"supervised_2_percent\":\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"semi\" or training_method == \"semi_supervised\" or training_method == \"semisupervised\":\n","        # SEMI SUPERVISED\n","        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"unsupervised\":\n","        # UNSUPERVISED\n","        autoencoder.fit(x_train, x_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"semi_sup_first\":\n","        autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","        autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=epochs, batch_size=32, shuffle=True, callbacks=[hist],\n","                        verbose=verbosity)\n","    elif training_method == \"semi_mixed\":\n","        for i in range(epochs):\n","            autoencoder.fit(x_train_nolabel, x_train_nolabel, epochs=1, batch_size=32, shuffle=True, callbacks=[hist],\n","                            verbose=verbosity)\n","            autoencoder.fit(x_train, y_train, epochs=1, batch_size=32, shuffle=True, callbacks=[hist],\n","                            verbose=verbosity)\n","    else:\n","        raise Exception(\"Invalid training method\")\n","    return autoencoder\n","\n","\n","def measure_performance(df, hard_evidence, autoencoder, sizes_sorted,rows,full_string,original_database):\n","    test_data = df.head(rows)\n","\n","    verify_data = hard_evidence.iloc[test_data.index]\n","    results = pd.DataFrame(autoencoder.predict(test_data))\n","    \n","    results.to_csv(\"databases/\" + full_string + \"/post_cleaning\" + gpu_string + \".csv\")\n","\n","    i = 0\n","    distances_before = []\n","    distances_after = []\n","    flip_TP, flip_TN, flip_FP, flip_FN = [],[],[],[]\n","    entropy_before, entropy_after = [],[]\n","\n","    cleaned_database_non_pdb = pd.DataFrame().reindex_like(original_database)\n","\n","    for column_index,size in enumerate(sizes_sorted):\n","        ground_truth_attribute = verify_data.iloc[:, i:i + size]\n","        cleaned_attribute = results.iloc[:, i:i + size]\n","        dirty_attribute = test_data.iloc[:, i:i + size]\n","\n","        dist_before = prob_distance(ground_truth_attribute, dirty_attribute)\n","        dist_after = prob_distance(ground_truth_attribute, cleaned_attribute)\n","\n","        distances_before.append(np.nansum(dist_before))\n","        distances_after.append(np.nansum(dist_after))\n","\n","        # going back to actual data instead of probabilities to see if values changed\n","        ground_truth_val = np.argmax(ground_truth_attribute.values,1)\n","        clean_val = np.argmax(cleaned_attribute.values,1)\n","        dirty_val = np.argmax(dirty_attribute.values,1)\n","\n","        ground_truth_missing = np.max(ground_truth_attribute.values,1) == np.min(ground_truth_attribute.values,1)\n","        clean_missing = np.max(cleaned_attribute.values,1) == np.min(cleaned_attribute.values,1)\n","        dirty_missing = np.max(dirty_attribute.values,1) == np.min(dirty_attribute.values,1)\n","\n","        # If we are working on data where no noise was added at all, then F1 and accuracy scores and such are not applicable\n","        # We can use TP and FN to pass upwards how many values were flipped\n","        if verify_data.equals(test_data):\n","            # Changed values & missing entries made non-missing\n","            TP = np.count_nonzero((ground_truth_missing & ~clean_missing) | (  (~ground_truth_missing & ~clean_missing)  &    (ground_truth_val != clean_val)))\n","            # Value to missing\n","            FP=np.count_nonzero(~ground_truth_missing & clean_missing)\n","            # Missing to missing\n","            FN=np.count_nonzero(ground_truth_missing & clean_missing)\n","            # Value stayed the same\n","            TN=np.count_nonzero((~ground_truth_missing & ~clean_missing)  & (ground_truth_val==clean_val))\n","        else:\n","            # Don't count instances where ground truth was missing, because we simply have no idea.\n","            # True positive: Was missing or wrong, now correct\n","            TP = np.count_nonzero( ~ground_truth_missing & ( ((ground_truth_val != dirty_val) | dirty_missing) & (ground_truth_val == clean_val)) )\n","            # False positive: Was correct, now missing or wrong\n","            FP = np.count_nonzero( ~ground_truth_missing & ( (ground_truth_val == dirty_val) & ((ground_truth_val != clean_val) | clean_missing)) )\n","            # False negative: Was incorrect/missing and still is\n","            FN = np.count_nonzero( ~ground_truth_missing & ( ((ground_truth_val != dirty_val) | dirty_missing) & ((ground_truth_val != clean_val) | clean_missing)) )\n","            # True negative: was correct and stayed correct\n","            TN = np.count_nonzero( ~ground_truth_missing & ( (ground_truth_val == dirty_val) & (ground_truth_val == clean_val)) )\n","\n","        flip_TP.append(TP)\n","        flip_FN.append(FN)\n","        flip_FP.append(FP)\n","        flip_TN.append(TN)\n","\n","        entropy_before_cleaning_per_row = scipy.stats.entropy(dirty_attribute,axis=1).sum()\n","        entropy_after_cleaning_per_row = scipy.stats.entropy(cleaned_attribute,axis=1).sum()\n","\n","        entropy_before.append(entropy_before_cleaning_per_row)\n","        entropy_after.append(entropy_after_cleaning_per_row)\n","        \n","\n","        cleaned_database_non_pdb.iloc[:,column_index] = clean_max\n","\n","        i += size\n","\n","    cleaned_database_non_pdb.to_csv(\"databases/\" + full_string + \"/post_cleaning_non_pdb\" + gpu_string + \".csv\")\n","\n","    JSD_before = np.nansum(distances_before)\n","    JSD_after = np.nansum(distances_after)\n","\n","    flip_TP = np.nansum(flip_TP)\n","    flip_FN = np.nansum(flip_FN)\n","    flip_FP = np.nansum(flip_FP)\n","    flip_TN = np.nansum(flip_TN)\n","\n","    entropy_before = np.nansum(entropy_before)\n","    entropy_after = np.nansum(entropy_after)\n","\n","    # TODO sum entropy\n","\n","    # noise_left = 100 * avg_distance_after / avg_distance_before\n","    return JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after\n","\n","\n","def custom_loss(y_true, y_pred, sizes_sorted, loss_func):\n","    i = 0\n","    total_loss = 0\n","    if loss_func == 'JSD':\n","        loss_func = JSD\n","    elif loss_func == \"CCE\":\n","        loss_func = keras.losses.categorical_crossentropy\n","    else:\n","        loss_func = keras.losses.get(loss_func)\n","\n","    loss_list = []\n","\n","    for size in sizes_sorted:\n","        new_loss = loss_func(y_true[:, i:i + size], y_pred[:, i:i + size])\n","        loss_list.append(new_loss)\n","        i += size\n","    good_loss = tf.math.add_n(loss_list)\n","    return good_loss\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# -------------\n","def run_experiment(full_string=None,epochs=epochs_default, use_previous_df=False, BN_size=BN_size_default,\n","                   sampling_density=sampling_density_default, mu=mu_default, sigma=sigma_default,\n","                   activation_types=activation_types_default, hidden_layers=hidden_layers_default,\n","                   encoding_dim=encoding_dim_default, loss_function=loss_function_default,\n","                   training_method=training_method_default, activity_regularizer=activity_regularizer_default,\n","                   input_layer_type=input_layer_type_default, labeled_data_percentage=labeled_data_percentage_default,\n","                   VAE=VAE_default, CNN=CNN_default, kernel_landmarks=kernel_landmarks_default,\n","                   CNN_layers=CNN_layers_default, CNN_filters=CNN_filters_default,\n","                   CNN_kernel_size=CNN_kernel_size_default, gaussian_noise_sigma=gaussian_noise_sigma_default,\n","                   use_gaussian_noise=use_gaussian_noise_default, use_missing_entry=use_missing_entry_default,\n","                   missing_entry_prob=missing_entry_prob_default,rows=rows_default,use_file=use_file_default):\n","    if TEST_RUN_EPOCHS:\n","        epochs = TEST_RUN_EPOCH_NR\n","\n","    if use_file is None:\n","        bn = make_bn(BN_size, sampling_density)\n","        df, hard_evidence, sizes_sorted, gaussian_noise_layer_sigma_new,original_database = make_df(use_file, bn, mu, sigma, use_gaussian_noise, use_missing_entry,\n","                                              missing_entry_prob,rows,full_string,sampling_density,gaussian_noise_sigma)\n","    else:\n","        df, hard_evidence, sizes_sorted, gaussian_noise_layer_sigma_new,original_database = make_df(use_file, None, mu, sigma, use_gaussian_noise, use_missing_entry,\n","                                              missing_entry_prob,rows,full_string,sampling_density,gaussian_noise_sigma)\n","\n","    if loss_function != 'MSE':\n","        old_loss = loss_function[:]\n","        loss_function = lambda y_true, y_pred: custom_loss(y_true, y_pred, sizes_sorted, old_loss)\n","\n","    autoencoder = train_network(epochs, df, hard_evidence, activation_types, hidden_layers, encoding_dim, sizes_sorted,\n","                                loss_function, training_method, activity_regularizer, input_layer_type,\n","                                labeled_data_percentage, VAE, CNN, kernel_landmarks, CNN_layers, CNN_filters,\n","                                CNN_kernel_size, gaussian_noise_layer_sigma_new)\n","    JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after = measure_performance(df, hard_evidence, autoencoder, sizes_sorted,rows,full_string,original_database)\n","\n","\n","    del autoencoder\n","    gc.collect()\n","    keras.backend.clear_session()\n","    return JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after\n","\n","\n","runs = 0\n","lowest_results = 0\n","\n","while lowest_results < 10:\n","    lowest_results = min([len(x) for x in experiment_config_JSD_after])\n","    lowest_results_forcpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) < 100])\n","    lowest_results_forgpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) >= 100])\n","    highest_results = max([len(x) for x in experiment_config_JSD_after])\n","    print(\"\\n\\n----- LOWEST RESULTS: \" + str(lowest_results) + \", HIGHEST: \" + str(highest_results) + \" ------\\n\\n\")\n","    for i in (range(len(experiments))):\n","        experiment = experiments[i]\n","        x = experiment\n","        previous_runs = len(experiment_config_JSD_after[experiment['mapping']])\n","        if previous_runs == lowest_results:\n","            # if previous_runs==lowest_results_forcpu and (x['config']['sampling_density']*x['config']['BN_size'])<100:\n","            # if previous_runs==lowest_results_forgpu and (x['config']['sampling_density']*x['config']['BN_size'])>=100:\n","            # if runs == 0:\n","            #     print(i)\n","            if runs % 10 == 0 and runs > 0:\n","                clear_output(wait=True)\n","            JSD_before, JSD_after, flip_TP, flip_TN, flip_FP, flip_FN, entropy_before, entropy_after = run_experiment(experiment['full_string'],**experiment['config'])\n","\n","            if JSD_before > 0:\n","                JSD_reduction = 100 - ((JSD_after / JSD_before)*100)\n","            elif JSD_before == JSD_after:\n","                JSD_reduction = 0\n","            else:\n","                JSD_reduction = -np.inf\n","            accuracy = (flip_TP+flip_TN) / (flip_TP+flip_TN+flip_FP+flip_FN)\n","            f1_score = (flip_TP) / (flip_TP + 0.5*(flip_FP + flip_FN))\n","            if entropy_before > 0:\n","                entropy_reduction = 100 - ((entropy_after / entropy_before)*100)\n","            elif entropy_before == entropy_after:\n","                entropy_reduction = 0\n","            else:\n","                entropy_reduction = -np.inf\n","            \n","            result_prints = pd.DataFrame([*experiment['full_string_list'],JSD_reduction,accuracy,f1_score,entropy_reduction]).T\n","            result_prints.columns = [\"Base config\",\"Parameter\",\"Value\",\"Noise reduction\",\"Accuracy\",\"F1 score\",\"Entropy reduction\"]\n","            result_prints.index=[runs]\n","            display(result_prints)\n","            # print(\"(\" + str(i) + \") \" + experiment['full_string'] + \";    \" + \"Q: \" + str(JSD_reduction) + \" ACC: \" + str(accuracy) + \" F1: \" + str(f1_score) + \" H_red: \" + str(entropy_reduction))\n","\n","            experiment_config_JSD_before[experiment['mapping']].append(JSD_before)\n","            experiment_config_JSD_after[experiment['mapping']].append(JSD_after)\n","\n","            experiment_config_flip_TP[experiment['mapping']].append(flip_TP)\n","            experiment_config_flip_TN[experiment['mapping']].append(flip_TN)\n","            experiment_config_flip_FP[experiment['mapping']].append(flip_FP)\n","            experiment_config_flip_FN[experiment['mapping']].append(flip_FN)\n","\n","            experiment_config_entropy_before[experiment['mapping']].append(entropy_before)\n","            experiment_config_entropy_after[experiment['mapping']].append(entropy_after)\n","\n","            #             experiment_configs_and_results[freeze(experiment['config'])].append(result)\n","\n","\n","            experiment_config_JSD_before_csv = [[experiment_config_strings[i]] + experiment_config_JSD_before[i] for i in range(len(experiment_config_JSD_before))]\n","            experiment_config_JSD_after_csv = [[experiment_config_strings[i]] + experiment_config_JSD_after[i] for i in range(len(experiment_config_JSD_after))]\n","\n","            experiment_config_flip_TP_csv = [[experiment_config_strings[i]] + experiment_config_flip_TP[i] for i in range(len(experiment_config_flip_TP))]\n","            experiment_config_flip_TN_csv = [[experiment_config_strings[i]] + experiment_config_flip_TN[i] for i in range(len(experiment_config_flip_TN))]\n","            experiment_config_flip_FP_csv = [[experiment_config_strings[i]] + experiment_config_flip_FP[i] for i in range(len(experiment_config_flip_FP))]\n","            experiment_config_flip_FN_csv = [[experiment_config_strings[i]] + experiment_config_flip_FN[i] for i in range(len(experiment_config_flip_FN))]\n","\n","            experiment_config_entropy_before_csv = [[experiment_config_strings[i]] + experiment_config_entropy_before[i] for i in range(len(experiment_config_entropy_before))]\n","            experiment_config_entropy_after_csv = [[experiment_config_strings[i]] + experiment_config_entropy_after[i] for i in range(len(experiment_config_entropy_after))]\n","\n","            with DelayedKeyboardInterrupt():\n","                with open(\"results/experiment_config_JSD_before\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_JSD_before_csv)\n","                with open(\"results/experiment_config_JSD_after\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_JSD_after_csv)\n","                with open(\"results/experiment_config_flip_TP\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_TP_csv)\n","                with open(\"results/experiment_config_flip_TN\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_TN_csv)\n","                with open(\"results/experiment_config_flip_FP\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_FP_csv)\n","                with open(\"results/experiment_config_flip_FN\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_flip_FN_csv)\n","                with open(\"results/experiment_config_entropy_before\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_entropy_before_csv)\n","                with open(\"results/experiment_config_entropy_after\" + gpu_string + \".csv\", \"w\", newline=\"\") as f: csv.writer(f).writerows(experiment_config_entropy_after_csv)\n","                with open(\"experiments\" + gpu_string, \"wb\") as dill_file:\n","                    dill.dump(experiments, dill_file)\n","\n","            runs += 1\n","    lowest_results = min([len(x) for x in experiment_config_JSD_after])\n","    lowest_results_forcpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) < 100])\n","    lowest_results_forgpu = min([len(experiment_config_JSD_after[x['mapping']]) for x in experiments if\n","                                 (x['config']['sampling_density'] * x['config']['BN_size']) >= 100])\n","\n","\n"],"outputs":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}